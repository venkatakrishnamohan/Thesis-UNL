{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/envs/textClassifier/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "#import cPickle\n",
    "from collections import defaultdict\n",
    "import re\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "import sys\n",
    "import os\n",
    "\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer, text_to_word_sequence\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils.np_utils import to_categorical\n",
    "\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Dense, Input, Flatten\n",
    "from keras.layers import Conv1D, MaxPooling1D, Embedding, Dropout, LSTM, GRU, Bidirectional, TimeDistributed, SpatialDropout1D\n",
    "from keras.models import Model\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "\n",
    "from keras import backend as K\n",
    "from keras.engine.topology import Layer, InputSpec\n",
    "from keras import initializers, regularizers, constraints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "MAX_SENT_LENGTH = 60\n",
    "MAX_SENTS = 15\n",
    "MAX_NB_WORDS = 1000\n",
    "EMBEDDING_DIM = 100\n",
    "VALIDATION_SPLIT = 0.0\n",
    "\n",
    "\n",
    "def clean_str(string):\n",
    "    \"\"\"\n",
    "    Tokenization/string cleaning for dataset\n",
    "    Every dataset is lower cased except\n",
    "    \"\"\"\n",
    "    string = re.sub(r\"\\\\\", \"\", string)\n",
    "    string = re.sub(r\"\\'\", \"\", string)\n",
    "    string = re.sub(r\"\\\"\", \"\", string)\n",
    "    return string.strip().lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9, 5)\n"
     ]
    }
   ],
   "source": [
    "data_train = pd.read_csv('document similarity/document_similarity.csv', sep=',')\n",
    "print(data_train.shape)\n",
    "\n",
    "data_train.dropna(inplace=True)\n",
    "\n",
    "#data_train['cat_id'] = 0\n",
    "#for i,row in data_train.iterrows():\n",
    "#    if row['category'] == 'unrest':\n",
    "#        data_train.set_value(i,'cat_id',0)\n",
    "#    else:\n",
    "#        data_train.set_value(i,'cat_id',1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>category</th>\n",
       "      <th>content</th>\n",
       "      <th>doc2vec</th>\n",
       "      <th>doc2veccosine</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>movie</td>\n",
       "      <td>Padmaavat: Why a Bollywood epic has sparked fi...</td>\n",
       "      <td>[ -9.05430984   1.71762216   1.08929253  -3.22...</td>\n",
       "      <td>[ 1.          0.94052142  0.9796434   0.926054...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>lokpal</td>\n",
       "      <td>Anna Hazare on fast-unto-death demanding Jan L...</td>\n",
       "      <td>[ -4.08943748e+00   3.65195364e-01   4.3100047...</td>\n",
       "      <td>[ 0.94052142  1.          0.97000271  0.940714...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>movie</td>\n",
       "      <td>A Film Has Inflamed Indians. But Moviegoers Mo...</td>\n",
       "      <td>[ -8.49335098   0.55525446   1.04886711  -3.75...</td>\n",
       "      <td>[ 0.9796434   0.97000271  1.          0.946262...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>movie</td>\n",
       "      <td>Indian police detain dozens protesting against...</td>\n",
       "      <td>[ -6.10597134   0.87642241   0.60653818  -3.09...</td>\n",
       "      <td>[ 0.92605472  0.9407143   0.94626272  0.999999...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>lokpal</td>\n",
       "      <td>Thousands Back Antigraft Hunger Strike in New ...</td>\n",
       "      <td>[-11.33139229   2.65446663   1.2543447   -0.94...</td>\n",
       "      <td>[ 0.8824504   0.86918712  0.90175384  0.953397...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0 category                                            content  \\\n",
       "0           0    movie  Padmaavat: Why a Bollywood epic has sparked fi...   \n",
       "1           1   lokpal  Anna Hazare on fast-unto-death demanding Jan L...   \n",
       "2           2    movie  A Film Has Inflamed Indians. But Moviegoers Mo...   \n",
       "3           3    movie  Indian police detain dozens protesting against...   \n",
       "4           4   lokpal  Thousands Back Antigraft Hunger Strike in New ...   \n",
       "\n",
       "                                             doc2vec  \\\n",
       "0  [ -9.05430984   1.71762216   1.08929253  -3.22...   \n",
       "1  [ -4.08943748e+00   3.65195364e-01   4.3100047...   \n",
       "2  [ -8.49335098   0.55525446   1.04886711  -3.75...   \n",
       "3  [ -6.10597134   0.87642241   0.60653818  -3.09...   \n",
       "4  [-11.33139229   2.65446663   1.2543447   -0.94...   \n",
       "\n",
       "                                       doc2veccosine  \n",
       "0  [ 1.          0.94052142  0.9796434   0.926054...  \n",
       "1  [ 0.94052142  1.          0.97000271  0.940714...  \n",
       "2  [ 0.9796434   0.97000271  1.          0.946262...  \n",
       "3  [ 0.92605472  0.9407143   0.94626272  0.999999...  \n",
       "4  [ 0.8824504   0.86918712  0.90175384  0.953397...  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/envs/textClassifier/lib/python3.6/site-packages/keras_preprocessing/text.py:175: UserWarning: The `nb_words` argument in `Tokenizer` has been renamed `num_words`.\n",
      "  warnings.warn('The `nb_words` argument in `Tokenizer` '\n"
     ]
    }
   ],
   "source": [
    "from nltk import tokenize\n",
    "\n",
    "reviews = []\n",
    "labels = []\n",
    "texts = []\n",
    "i=0\n",
    "\n",
    "for idx in data_train['Unnamed: 0']:\n",
    "    i = idx\n",
    "    text = BeautifulSoup(data_train.content[idx])\n",
    "    text = clean_str(text.get_text())\n",
    "    texts.append(text)\n",
    "    sentences = tokenize.sent_tokenize(text)\n",
    "    reviews.append(sentences)\n",
    "\n",
    "    #labels.append(data_train.cat_id[idx])\n",
    "\n",
    "tokenizer = Tokenizer(nb_words=MAX_NB_WORDS)\n",
    "tokenizer.fit_on_texts(texts)\n",
    "\n",
    "data = np.zeros((len(texts), MAX_SENTS, MAX_SENT_LENGTH), dtype='int32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total 1553 unique tokens.\n",
      "Shape of data tensor: (9, 15, 60)\n"
     ]
    }
   ],
   "source": [
    "for i, sentences in enumerate(reviews):\n",
    "    for j, sent in enumerate(sentences):\n",
    "        if j < MAX_SENTS:\n",
    "            # convert sentence to words\n",
    "            wordTokens = text_to_word_sequence(sent)\n",
    "            k = 0\n",
    "            for _, word in enumerate(wordTokens):\n",
    "                # If the number of words in a sentence is less than 100 and it's index is < 20000 then assign it.\n",
    "                if k < MAX_SENT_LENGTH and tokenizer.word_index[word] < MAX_NB_WORDS:\n",
    "                    data[i, j, k] = tokenizer.word_index[word]\n",
    "                    k = k + 1\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "print('Total %s unique tokens.' % len(word_index))\n",
    "\n",
    "#labels = to_categorical(np.asarray(labels))\n",
    "print('Shape of data tensor:', data.shape)\n",
    "#print('Shape of label tensor:', labels.shape)\n",
    "\n",
    "#indices = np.arange(data.shape[0])\n",
    "#np.random.shuffle(indices)\n",
    "#data = data[indices]\n",
    "#labels = labels[indices]\n",
    "#nb_validation_samples = int(VALIDATION_SPLIT * data.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of positive and negative reviews in traing and validation set\n",
      "[35262. 35140.]\n",
      "[8757. 8843.]\n",
      "Total 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "x_train = data[:-nb_validation_samples]\n",
    "y_train = labels[:-nb_validation_samples]\n",
    "x_val = data[-nb_validation_samples:]\n",
    "y_val = labels[-nb_validation_samples:]\n",
    "\n",
    "print('Number of positive and negative reviews in traing and validation set')\n",
    "print(y_train.sum(axis=0))\n",
    "print(y_val.sum(axis=0))\n",
    "\n",
    "GLOVE_DIR = \"/Users/venkatakrishnamohansunkara/Desktop/DM/textClassifier\"\n",
    "embeddings_index = {}\n",
    "f = open(os.path.join(GLOVE_DIR, 'glove.6B.100d.txt'))\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "print('Total %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_data = data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "embedding_matrix = np.random.random((len(word_index) + 1, EMBEDDING_DIM))\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "         #words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "    else:\n",
    "        embedding_matrix[i] = 100*[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "embedding_layer = Embedding(len(word_index) + 1,\n",
    "                            EMBEDDING_DIM,\n",
    "                            weights=[embedding_matrix],\n",
    "                            input_length=MAX_SENT_LENGTH,\n",
    "                            trainable=True,\n",
    "                            mask_zero=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def dot_product(x, kernel):\n",
    "    \"\"\"\n",
    "    Wrapper for dot product operation, in order to be compatible with both\n",
    "    Theano and Tensorflow\n",
    "    Args:\n",
    "        x (): input\n",
    "        kernel (): weights\n",
    "    Returns:\n",
    "    \"\"\"\n",
    "    if K.backend() == 'tensorflow':\n",
    "        return K.squeeze(K.dot(x, K.expand_dims(kernel)), axis=-1)\n",
    "    else:\n",
    "        return K.dot(x, kernel)\n",
    "\n",
    "    \n",
    "class AttentionWithContext(Layer):\n",
    "    \"\"\"\n",
    "    Attention operation, with a context/query vector, for temporal data.\n",
    "    Supports Masking.\n",
    "    Follows the work of Yang et al. [https://www.cs.cmu.edu/~diyiy/docs/naacl16.pdf]\n",
    "    \"Hierarchical Attention Networks for Document Classification\"\n",
    "    by using a context vector to assist the attention\n",
    "    # Input shape\n",
    "        3D tensor with shape: `(samples, steps, features)`.\n",
    "    # Output shape\n",
    "        2D tensor with shape: `(samples, features)`.\n",
    "    How to use:\n",
    "    Just put it on top of an RNN Layer (GRU/LSTM/SimpleRNN) with return_sequences=True.\n",
    "    The dimensions are inferred based on the output shape of the RNN.\n",
    "    Note: The layer has been tested with Keras 2.0.6\n",
    "    Example:\n",
    "        model.add(LSTM(64, return_sequences=True))\n",
    "        model.add(AttentionWithContext())\n",
    "        # next add a Dense layer (for classification/regression) or whatever...\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 W_regularizer=None, u_regularizer=None, b_regularizer=None,\n",
    "                 W_constraint=None, u_constraint=None, b_constraint=None,\n",
    "                 bias=True, **kwargs):\n",
    "\n",
    "        self.supports_masking = True\n",
    "        self.init = initializers.get('glorot_uniform')\n",
    "\n",
    "        self.W_regularizer = regularizers.get(W_regularizer)\n",
    "        self.u_regularizer = regularizers.get(u_regularizer)\n",
    "        self.b_regularizer = regularizers.get(b_regularizer)\n",
    "        \n",
    "        self.W_constraint = constraints.get(W_constraint)\n",
    "        self.u_constraint = constraints.get(u_constraint)\n",
    "        self.b_constraint = constraints.get(b_constraint)\n",
    "\n",
    "        self.bias = bias\n",
    "        super(AttentionWithContext, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape) == 3\n",
    "\n",
    "        self.W = self.add_weight((input_shape[-1], input_shape[-1],),\n",
    "                                 initializer=self.init,\n",
    "                                 name='{}_W'.format(self.name),\n",
    "                                 regularizer=self.W_regularizer,\n",
    "                                 constraint=self.W_constraint)\n",
    "        if self.bias:\n",
    "            self.b = self.add_weight((input_shape[-1],),\n",
    "                                     initializer='zero',\n",
    "                                     name='{}_b'.format(self.name),\n",
    "                                     regularizer=self.b_regularizer,\n",
    "                                     constraint=self.b_constraint)\n",
    "        self.u = self.add_weight((input_shape[-1],),\n",
    "                                 initializer=self.init,\n",
    "                                 name='{}_u'.format(self.name),\n",
    "                                 regularizer=self.u_regularizer,\n",
    "                                 constraint=self.u_constraint)\n",
    "\n",
    "        super(AttentionWithContext, self).build(input_shape)\n",
    "\n",
    "    def compute_mask(self, input, input_mask=None):\n",
    "        # do not pass the mask to the next layers\n",
    "        return None\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        uit = dot_product(x, self.W)\n",
    "\n",
    "        if self.bias:\n",
    "            uit += self.b\n",
    "\n",
    "        uit = K.tanh(uit)\n",
    "        ait = dot_product(uit, self.u)\n",
    "\n",
    "        #a = K.exp(ait)\n",
    "        a = K.softmax(ait)\n",
    "        # apply mask after the exp. will be re-normalized next\n",
    "        #if mask is not None:\n",
    "        #    # Cast the mask to floatX to avoid float64 upcasting in theano\n",
    "        #    a *= K.cast(mask, K.floatx())\n",
    "\n",
    "        # in some cases especially in the early stages of training the sum may be almost zero\n",
    "        # and this results in NaN's. A workaround is to add a very small positive number Îµ to the sum.\n",
    "        # a /= K.cast(K.sum(a, axis=1, keepdims=True), K.floatx())\n",
    "        a = K.expand_dims(a)\n",
    "        weighted_input = x * a\n",
    "        return K.sum(weighted_input, axis=1)\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape[0], input_shape[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         (None, 60)                0         \n",
      "_________________________________________________________________\n",
      "embedding_2 (Embedding)      (None, 60, 100)           40139200  \n",
      "_________________________________________________________________\n",
      "spatial_dropout1d_1 (Spatial (None, 60, 100)           0         \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 60, 200)           160800    \n",
      "_________________________________________________________________\n",
      "attention_with_context_1 (At (None, 200)               40400     \n",
      "=================================================================\n",
      "Total params: 40,340,400\n",
      "Trainable params: 40,340,400\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "input_layer = Input(shape=(MAX_SENTS,MAX_SENT_LENGTH),dtype='int32')\n",
    "sentence_input = Input(shape=(MAX_SENT_LENGTH,),dtype='int32')\n",
    "embedding_layer = Embedding(len(word_index) + 1,\n",
    "                            EMBEDDING_DIM,\n",
    "                            weights=[embedding_matrix],\n",
    "                            input_length=MAX_SENT_LENGTH,\n",
    "                            trainable=True,\n",
    "                            mask_zero=True)(sentence_input)\n",
    "drop1 = SpatialDropout1D(0.3)(embedding_layer)\n",
    "sent_lstm = Bidirectional(LSTM(100, name='blstm_1',\n",
    "        activation='tanh',\n",
    "        recurrent_activation='hard_sigmoid',\n",
    "        recurrent_dropout=0.0,\n",
    "        dropout=0.4,\n",
    "        kernel_initializer='glorot_uniform',\n",
    "        return_sequences=True),\n",
    "        merge_mode='concat')(drop1)\n",
    "\n",
    "sent_att_layer = AttentionWithContext()(sent_lstm)\n",
    "sentEncoder = Model(sentence_input, sent_att_layer)\n",
    "sentEncoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 15, 60)            0         \n",
      "_________________________________________________________________\n",
      "time_distributed_1 (TimeDist (None, 15, 200)           40340400  \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 15, 200)           0         \n",
      "_________________________________________________________________\n",
      "bidirectional_2 (Bidirection (None, 15, 200)           240800    \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 15, 200)           800       \n",
      "_________________________________________________________________\n",
      "attention_with_context_2 (At (None, 200)               40400     \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 2)                 402       \n",
      "=================================================================\n",
      "Total params: 40,622,802\n",
      "Trainable params: 40,622,402\n",
      "Non-trainable params: 400\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "textEncoder = TimeDistributed(sentEncoder)(input_layer)\n",
    "drop2 = Dropout(0.4)(textEncoder)\n",
    "\n",
    "lstm_1 = Bidirectional(LSTM(100, name='blstm_2',\n",
    "        activation='tanh',\n",
    "        recurrent_activation='hard_sigmoid',\n",
    "        recurrent_dropout=0.0,\n",
    "        dropout=0.4,\n",
    "        kernel_initializer='glorot_uniform',\n",
    "        return_sequences=True),\n",
    "        merge_mode='concat')(drop2)\n",
    "lstm_1 = BatchNormalization()(lstm_1)\n",
    "att_layer = AttentionWithContext()(lstm_1)\n",
    "drop3 = Dropout(0.5)(att_layer)\n",
    "predictions = Dense(2, activation='softmax')(drop3)\n",
    "model = Model(inputs=input_layer, outputs=predictions)\n",
    "model.summary()\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['acc'])\n",
    "#print(\"model fitting - Hierachical attention network\")\n",
    "#model.fit(x_train, y_train, validation_data=(x_val, y_val),\n",
    "#          nb_epoch=100, batch_size=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sentEncoder.load_weights('/Users/venkatakrishnamohansunkara/Desktop/DM/sent_encoder_lstm.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.load_weights('/Users/venkatakrishnamohansunkara/Desktop/DM/model_lstm.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get the 7000 unrest articles filtered and test it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_test = pd.read_csv('document similarity/document_similarity.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_test['HATT embeddings'] = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/envs/textClassifier/lib/python3.6/site-packages/ipykernel_launcher.py:5: FutureWarning: set_value is deprecated and will be removed in a future release. Please use .at[] or .iat[] accessors instead\n",
      "  \"\"\"\n"
     ]
    }
   ],
   "source": [
    "get_embeddings = K.function([model.layers[0].input, K.learning_phase()], [model.layers[6].output])\n",
    "for i in range(test_data.shape[0]):\n",
    "    location = i\n",
    "    embeddings = get_embeddings([test_data[location],0])\n",
    "    data_test.set_value(i,'HATT embeddings',np.asarray(embeddings[0][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[8.284906e-11, 1.000000e+00]], dtype=float32)]\n",
      "[1. 0.]\n",
      "(1, 15, 200)\n"
     ]
    }
   ],
   "source": [
    "location = 0\n",
    "# Get the prediction\n",
    "get_output = K.function([model.layers[0].input, K.learning_phase()], [model.layers[7].output])\n",
    "predictions = get_output([test_data[location],0])\n",
    "print(predictions)\n",
    "print(y_train[location])\n",
    "# Get output before the attention layer\n",
    "get_layer_output = K.function([model.layers[0].input, K.learning_phase()], [model.layers[4].output])\n",
    "out = get_layer_output([test_data[location],0])  # test mode\n",
    "print(out[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(70402, 2)"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#all(labels == [1,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "labels = y_train[location]\n",
    "if all(labels == [1,0]):\n",
    "    label = 'unrest';\n",
    "else:\n",
    "    label = 'other'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 0.], dtype=float32)"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if predictions[0][0][1]>predictions[0][0][0]:\n",
    "    prediction = 'other';\n",
    "else:\n",
    "    prediction = 'unrest';"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get the attention weights of the sentence network.\n",
    "eij = np.tanh(np.dot(out[0],model.layers[5].get_weights()[0]))\n",
    "t = np.dot(eij,model.layers[5].get_weights()[2])\n",
    "ai = np.exp(t)\n",
    "weights = ai/np.sum(ai)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 15)"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 15 weights for 15 sentences.\n",
    "weights.shape\n",
    "#np.sum(out[0]*weights,axis=1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(15, 60, 200)\n"
     ]
    }
   ],
   "source": [
    "# Get the attention weights of the word network for every sentence.\n",
    "get_layer_op_words = K.function([sentEncoder.layers[0].input, K.learning_phase()], [sentEncoder.layers[3].output])\n",
    "op_words = get_layer_op_words([x_train[location],0])\n",
    "print(op_words[0].shape)       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "weight_all_words = []\n",
    "for i in range(MAX_SENTS):\n",
    "    eij_words = np.tanh(np.dot(op_words[0][i],sentEncoder.layers[4].get_weights()[0]))\n",
    "    t_words = np.dot(eij_words,sentEncoder.layers[4].get_weights()[2])\n",
    "    ai_words = np.exp(t_words)\n",
    "    weights_words = ai_words/np.sum(ai_words)\n",
    "    weight_all_words.append(weights_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 60 weights for 60 words \n",
    "len(weight_all_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.10022662---coimbatore 0.06340147 unfazed 0.023951871 threats 0.02011564 sasikala 0.038772874 family 0.014973568 united 0.013860773 edappadi 0.014677104 k 0.01206006 palaniswamy 0.019769648 faction 0.012154868 dared 0.019205896 ttv 0.018676732 dhinakaran 0.007816201 bring 0.011013801 government 0.013180024 .\n",
      "0.058086295---mla 0.16205435 v 0.014369314 c 0.029780287 arukutty 0.031516973 said 0.022466492 sasikala 0.025386395 family 0.0066732434 members 0.015372422 would 0.009487164 allowed 0.008164034 continue 0.013582026 aiadmk 0.0340802 even 0.012857487 government 0.01051414 falls 0.007902034 .\n",
      "0.079827234---cost 0.005626709 sasikala 0.009243172 family 0.0023460703 members 0.0076649003 entertained 0.037988238 aiadmk 0.01693854 .\n",
      "0.3140566---even 0.007824148 government 0.008271808 falls 0.0052241683 allowed 0.0060349302 party 0.008252758 arukutty 0.02236415 told 0.010345576 reporters 0.008443309 coimbatore 0.016609147 wednesday 0.013680717 .\n",
      "0.14848755---let 0.009695693 dhinakaran 0.005857514 try 0.006821304 government 0.0118380375 said 0.013628674 .\n",
      "0.013831998---partymen 0.04489084 ready 0.0051207403 accept 0.011294883 sasikala 0.020400256 added 0.00461185 said 0.0049249064 space 0.008590198 family 0.005248692 members 0.0064282822 party 0.009947902 .\n",
      "0.022806544---get 0.0025708643 furious 0.024144502 even 0.009971876 see 0.008500328 sasikala 0.0110958405 s 0.007849857 photos 0.008402611 said 0.0134253735 .\n",
      "0.036810827---arukutty 0.029857742 first 0.007775902 among 0.00818658 mlas 0.031357754 support 0.0112072425 panneerselvam 0.057174757 rebelled 0.0061756256 eventually 0.0036475079 switched 0.0043701343 edappadi 0.01668327 palaniswami 0.017575454 camp 0.00272432 weeks 0.0027841758 merger 0.005095402 two 0.004828411 factions 0.012928796 dhinakaran 0.02089136 sacking 0.019526446 several 0.013691977 party 0.01216949 functionaries 0.016280193 including 0.013115046 rajya 0.025449496 sabha 0.018356552 mp 0.013821543 vaithilingam 0.03103642 mla 0.013245475 rajan 0.016364213 arukutty 0.00882984 said 0.013031247 dhinakaran 0.01918192 locus 0.028445922 remove 0.03035199 leaders 0.0235255 toiled 0.045692608 aiadmk 0.03655566 s 0.007411303 growth 0.012823611 three 0.006525127 decades 0.014839262 .\n",
      "0.11086788---daring 0.008810703 dhinakaran 0.01056793 prove 0.006214506 strength 0.01349934 general 0.00998127 council 0.019026833 said 0.009895481 mlas 0.04212035 camping 0.025076214 puducherry 0.008766004 return 0.0066971374 constituencies 0.007086458 immediately 0.013139711 serve 0.00455981 people 0.013705148 arukutty 0.018518992 also 0.012642861 slammed 0.02373654 sasikala 0.019861085 s 0.010463641 brother 0.013236175 v 0.014424067 k 0.015805343 airing 0.021553436 views 0.020576322 chief 0.020668082 minister 0.018095676 .\n",
      "0.030272586---rights 0.018301839 speak 0.031224139 internal 0.017450048 affairs 0.035555866 aiadmk 0.08420749 said 0.012145035 .\n",
      "0.020558784---dhinakaran 0.2836928 supporters 0.06581601 sampath 0.12050858 instigated 0.02872018 sasikala 0.028273666 family 0.0061098225 speak 0.009236342 aiadmk 0.012992659 leaders 0.008794973 added 0.0070235296 .\n",
      "0.015908254---stop 0.013796644 attacking 0.014213682 senior 0.025587983 leaders 0.027432263 aiadmk 0.08602768 said 0.012551541 .\n",
      "0.020704523---senior 0.01713848 leaders 0.010041187 chennai 0.015315171 like 0.0074904915 minister 0.0077737104 jayakumar 0.016592177 issuing 0.01910008 dhinakaran 0.016473262 days 0.012410114 critical 0.01585277 voice 0.062288854 coming 0.02142438 coimbatore 0.029049575 time 0.02345895 raised 0.027175158 many 0.011019336 eyebrows 0.02330775 .\n",
      "0.024296504---though 0.0058540995 arukutty 0.0039747995 claimed 0.0048846765 whatever 0.0068301633 said 0.006547058 personal 0.00460537 view 0.0053960932 party 0.0060691508 insiders 0.006353074 said 0.0056248913 would 0.0058329045 vocal 0.021591887 taking 0.0066512614 dhinakaran 0.021818355 without 0.0074592023 backing 0.013400159 party 0.005135532 heavyweights 0.026242632 in 0.008048849 short 0.060496967 sampath 0.027184354 met 0.010383196 reporters 0.0066098114 chennai 0.0074413996 gave 0.007858868 rebuttal 0.004838855 arukutty 0.014452136 .\n",
      "0.003257797---remain 0.0055850996 silent 0.029531974 dhinakaran 0.0455193 made 0.014796883 chief 0.039701078 minister 0.059516974 tamil 0.04179804 nadu 0.021093365 said 0.011217431 .\n"
     ]
    }
   ],
   "source": [
    "id2word = {v: k for k, v in word_index.items()}\n",
    "with open(\"Recent_Visualizations/visualization_test.html\", \"w\") as html_file:\n",
    "    html_file.write('<!DOCTYPE html>\\n')\n",
    "    html_file.write('<html>\\n')\n",
    "    html_file.write('<body>\\n')\n",
    "    sent_no = 0\n",
    "    html_file.write('<p>\\n')\n",
    "    html_file.write('<h3>Prediction: %s</h3>\\n' %prediction)\n",
    "    html_file.write('<h3>Label: %s</h3>\\n' %label)\n",
    "    html_file.write('</p>\\n')\n",
    "    for i in x_train[location]:\n",
    "        # Iterate through the words in the sentence\n",
    "        weights_words = weight_all_words[sent_no]\n",
    "        sent = ''\n",
    "        word_no =0\n",
    "        html_file.write('<font style=\"background-color: rgba(255, 0, 0, %f)\">%s</font>\\n' %(weights[0][sent_no],str(weights[0][sent_no])))\n",
    "        for j in i:\n",
    "            if j!=0:\n",
    "                alpha = weights_words[word_no]*40\n",
    "                word = id2word[j]\n",
    "                html_file.write('<font style=\"background-color: rgba(0, 0, 255, %f)\">%s</font>\\n' % (alpha, word))\n",
    "                sent+=id2word[j]+' '+str(weights_words[word_no])+' ' \n",
    "            word_no+=1\n",
    "        html_file.write('<br />\\n')\n",
    "        print(str(weights[0][sent_no])+'---'+sent+'.')\n",
    "        sent_no+=1\n",
    "    #print('<br />\\n')\n",
    "    #print('<br />\\n')\n",
    "    #print('<h5> Original Document<h5>\\n')\n",
    "    #print('<p>\\n')\n",
    "    #print('%s\\n' %data_train.iloc[location]['content'])\n",
    "    #print('</p>\\n')\n",
    "    html_file.write('</body>\\n')\n",
    "    html_file.write('</html>\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sentEncoder.layers[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_train[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_train.iloc[location]['Content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.evaluate(data,labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "labels = np.tile([0,1],(7061,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "labels[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#from keras.utils import plot_model\n",
    "#plot_model(model, to_file='model.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_test.to_csv('document similarity/document_similarity_hatt.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_test['hattcosine'] = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from numpy import dot\n",
    "from numpy.linalg import norm\n",
    "\n",
    "def cos_sim(a,b):\n",
    "    return dot(a, b)/(norm(a)*norm(b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = data_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/envs/textClassifier/lib/python3.6/site-packages/ipykernel_launcher.py:6: FutureWarning: set_value is deprecated and will be removed in a future release. Please use .at[] or .iat[] accessors instead\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "for i,row1 in df.iterrows():\n",
    "    l= []\n",
    "    for j,row2 in df.iterrows():\n",
    "        cs = cos_sim(row1['HATT embeddings'],row2['HATT embeddings'])\n",
    "        l+=[cs]\n",
    "    df.set_value(i,'hattcosine',np.asarray(l))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "[1.0000001  0.92749    0.90885884]\n",
      "[0 7 6]\n",
      "1\n",
      "[1.0000001  0.81181544 0.78600794]\n",
      "[1 8 0]\n",
      "2\n",
      "[1.         0.71136224 0.5640011 ]\n",
      "[2 1 8]\n",
      "3\n",
      "[1.0000001 0.9190015 0.911249 ]\n",
      "[3 6 8]\n",
      "4\n",
      "[0.99999994 0.7212733  0.7097385 ]\n",
      "[4 7 6]\n",
      "5\n",
      "[0.9999999  0.7778364  0.76642966]\n",
      "[5 8 1]\n",
      "6\n",
      "[0.9999999  0.9190015  0.90885884]\n",
      "[6 3 0]\n",
      "7\n",
      "[1.0000001 0.92749   0.9024751]\n",
      "[7 0 6]\n",
      "8\n",
      "[0.9999999 0.911249  0.8382874]\n",
      "[8 3 7]\n"
     ]
    }
   ],
   "source": [
    "for i,row in df.iterrows():\n",
    "    print(i);\n",
    "    r = np.sort(row['hattcosine'])[::-1][0:9]\n",
    "    r2 = np.argsort(row['hattcosine'])[::-1][0:9]\n",
    "    print(r)\n",
    "    print(r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (textClassifier)",
   "language": "python",
   "name": "textclassifier"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
