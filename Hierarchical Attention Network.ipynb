{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/envs/textClassifier/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "#import cPickle\n",
    "from collections import defaultdict\n",
    "import re\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "import sys\n",
    "import os\n",
    "\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer, text_to_word_sequence\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils.np_utils import to_categorical\n",
    "\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Dense, Input, Flatten\n",
    "from keras.layers import Conv1D, MaxPooling1D, Embedding, Dropout, LSTM, GRU, Bidirectional, TimeDistributed, SpatialDropout1D\n",
    "from keras.models import Model\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "\n",
    "from keras import backend as K\n",
    "from keras.engine.topology import Layer, InputSpec\n",
    "from keras import initializers, regularizers, constraints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "MAX_SENT_LENGTH = 60\n",
    "MAX_SENTS = 15\n",
    "MAX_NB_WORDS = 20000\n",
    "EMBEDDING_DIM = 100\n",
    "VALIDATION_SPLIT = 0.2\n",
    "\n",
    "\n",
    "def clean_str(string):\n",
    "    \"\"\"\n",
    "    Tokenization/string cleaning for dataset\n",
    "    Every dataset is lower cased except\n",
    "    \"\"\"\n",
    "    string = re.sub(r\"\\\\\", \"\", string)\n",
    "    string = re.sub(r\"\\'\", \"\", string)\n",
    "    string = re.sub(r\"\\\"\", \"\", string)\n",
    "    return string.strip().lower()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(17575, 7)\n"
     ]
    }
   ],
   "source": [
    "data_train = pd.read_csv('/Users/venkatakrishnamohansunkara/Desktop/DM/classification_data.csv', sep=',')\n",
    "print(data_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Content</th>\n",
       "      <th>sentiment_compound</th>\n",
       "      <th>sentiment_neu</th>\n",
       "      <th>sentiment_neg</th>\n",
       "      <th>sentiment_pos</th>\n",
       "      <th>Category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>GANDHINAGAR: The state government's vaunted 'K...</td>\n",
       "      <td>0.8795</td>\n",
       "      <td>0.939</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.050</td>\n",
       "      <td>unrest</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>LUCKNOW: A committed RSS pracharak since 1977,...</td>\n",
       "      <td>-0.1500</td>\n",
       "      <td>0.852</td>\n",
       "      <td>0.070</td>\n",
       "      <td>0.078</td>\n",
       "      <td>unrest</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>CHENNAI: As news of the violence spread, the a...</td>\n",
       "      <td>-0.9971</td>\n",
       "      <td>0.747</td>\n",
       "      <td>0.220</td>\n",
       "      <td>0.032</td>\n",
       "      <td>unrest</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>BHIWANDI: Three boys have been rescued from th...</td>\n",
       "      <td>-0.9919</td>\n",
       "      <td>0.844</td>\n",
       "      <td>0.119</td>\n",
       "      <td>0.037</td>\n",
       "      <td>unrest</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>CHANDIGARH: As chief of internal security and ...</td>\n",
       "      <td>0.8983</td>\n",
       "      <td>0.861</td>\n",
       "      <td>0.061</td>\n",
       "      <td>0.078</td>\n",
       "      <td>unrest</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                            Content  \\\n",
       "0           0  GANDHINAGAR: The state government's vaunted 'K...   \n",
       "1           1  LUCKNOW: A committed RSS pracharak since 1977,...   \n",
       "2           2  CHENNAI: As news of the violence spread, the a...   \n",
       "3           3  BHIWANDI: Three boys have been rescued from th...   \n",
       "4           4  CHANDIGARH: As chief of internal security and ...   \n",
       "\n",
       "   sentiment_compound  sentiment_neu  sentiment_neg  sentiment_pos Category  \n",
       "0              0.8795          0.939          0.012          0.050   unrest  \n",
       "1             -0.1500          0.852          0.070          0.078   unrest  \n",
       "2             -0.9971          0.747          0.220          0.032   unrest  \n",
       "3             -0.9919          0.844          0.119          0.037   unrest  \n",
       "4              0.8983          0.861          0.061          0.078   unrest  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_train.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/envs/textClassifier/lib/python3.6/site-packages/ipykernel_launcher.py:4: FutureWarning: set_value is deprecated and will be removed in a future release. Please use .at[] or .iat[] accessors instead\n",
      "  after removing the cwd from sys.path.\n",
      "/anaconda3/envs/textClassifier/lib/python3.6/site-packages/ipykernel_launcher.py:6: FutureWarning: set_value is deprecated and will be removed in a future release. Please use .at[] or .iat[] accessors instead\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "data_train['cat_id'] = 0\n",
    "for i,row in data_train.iterrows():\n",
    "    if row['Category'] == 'unrest':\n",
    "        data_train.set_value(i,'cat_id',0)\n",
    "    else:\n",
    "        data_train.set_value(i,'cat_id',1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/envs/textClassifier/lib/python3.6/site-packages/keras_preprocessing/text.py:175: UserWarning: The `nb_words` argument in `Tokenizer` has been renamed `num_words`.\n",
      "  warnings.warn('The `nb_words` argument in `Tokenizer` '\n"
     ]
    }
   ],
   "source": [
    "from nltk import tokenize\n",
    "\n",
    "reviews = []\n",
    "labels = []\n",
    "texts = []\n",
    "i=0\n",
    "\n",
    "for idx in data_train['Unnamed: 0']:\n",
    "    i = idx\n",
    "    text = BeautifulSoup(data_train.Content[idx])\n",
    "    text = clean_str(text.get_text())\n",
    "    texts.append(text)\n",
    "    sentences = tokenize.sent_tokenize(text)\n",
    "    reviews.append(sentences)\n",
    "\n",
    "    labels.append(data_train.cat_id[idx])\n",
    "\n",
    "tokenizer = Tokenizer(nb_words=MAX_NB_WORDS)\n",
    "tokenizer.fit_on_texts(texts)\n",
    "\n",
    "data = np.zeros((len(texts), MAX_SENTS, MAX_SENT_LENGTH), dtype='int32')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# texts have the entire content as an entry. (25000,1)\n",
    "# reviews have a list of sentences as an entry. (25000,number_of_sentences)\n",
    "# labels have the label of a document. (25000,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total 100277 unique tokens.\n",
      "Shape of data tensor: (17467, 15, 60)\n",
      "Shape of label tensor: (17467, 2)\n"
     ]
    }
   ],
   "source": [
    "for i, sentences in enumerate(reviews):\n",
    "    for j, sent in enumerate(sentences):\n",
    "        if j < MAX_SENTS:\n",
    "            # convert sentence to words\n",
    "            wordTokens = text_to_word_sequence(sent)\n",
    "            k = 0\n",
    "            for _, word in enumerate(wordTokens):\n",
    "                # If the number of words in a sentence is less than 100 and it's index is < 20000 then assign it.\n",
    "                if k < MAX_SENT_LENGTH and tokenizer.word_index[word] < MAX_NB_WORDS:\n",
    "                    data[i, j, k] = tokenizer.word_index[word]\n",
    "                    k = k + 1\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "print('Total %s unique tokens.' % len(word_index))\n",
    "\n",
    "labels = to_categorical(np.asarray(labels))\n",
    "print('Shape of data tensor:', data.shape)\n",
    "print('Shape of label tensor:', labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Every document is converted to a 3d array. \n",
    "# 1st dimension is the index of the document.\n",
    "# 2nd dimension is the number of sentences in the document (Atmost 15 sentences, rest ignored)\n",
    "# 3rd dimension is the number of words in each sentence (Atmost 200 words, rest ignored)\n",
    "# The words are converted into indices and atmost 20000 unique words are allowed (rest ignored)\n",
    "# word index is a dictionary mapping from a word to a unique index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of positive and negative reviews in traing and validation set\n",
      "[6871. 7103.]\n",
      "[1704. 1789.]\n"
     ]
    }
   ],
   "source": [
    "indices = np.arange(data.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "data = data[indices]\n",
    "labels = labels[indices]\n",
    "nb_validation_samples = int(VALIDATION_SPLIT * data.shape[0])\n",
    "\n",
    "x_train = data[:-nb_validation_samples]\n",
    "y_train = labels[:-nb_validation_samples]\n",
    "x_val = data[-nb_validation_samples:]\n",
    "y_val = labels[-nb_validation_samples:]\n",
    "\n",
    "print('Number of positive and negative reviews in traing and validation set')\n",
    "print(y_train.sum(axis=0))\n",
    "print(y_val.sum(axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#20000 are used for training and the remaining 5000 are used for validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "GLOVE_DIR = \"/Users/venkatakrishnamohansunkara/Desktop/DM/textClassifier\"\n",
    "embeddings_index = {}\n",
    "f = open(os.path.join(GLOVE_DIR, 'glove.6B.100d.txt'))\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "print('Total %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# embeddings_index is a dictionary mapping from a word to a 100 dimensional vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# building Hierachical Attention network\n",
    "# Embedding matrix should consist of all the unique words (which is the lenght of word index) as rows and their embeddings as columns (100 dimensions)\n",
    "embedding_matrix = np.random.random((len(word_index) + 1, EMBEDDING_DIM))\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "         #words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "    else:\n",
    "        embedding_matrix[i] = 100*[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Now embedding matrix consists of 81503 rows and 100 columns.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# First words (a sentence of 100 words) are provided to the embedding layer and are converted to 100 dim vector using the embedding matrix.\n",
    "# Here the embedding matrix is learnt during the training process.\n",
    "# First argument is the size of the vocabulary.\n",
    "embedding_layer = Embedding(len(word_index) + 1,\n",
    "                            EMBEDDING_DIM,\n",
    "                            weights=[embedding_matrix],\n",
    "                            input_length=MAX_SENT_LENGTH,\n",
    "                            trainable=True,\n",
    "                            mask_zero=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def dot_product(x, kernel):\n",
    "    \"\"\"\n",
    "    Wrapper for dot product operation, in order to be compatible with both\n",
    "    Theano and Tensorflow\n",
    "    Args:\n",
    "        x (): input\n",
    "        kernel (): weights\n",
    "    Returns:\n",
    "    \"\"\"\n",
    "    if K.backend() == 'tensorflow':\n",
    "        return K.squeeze(K.dot(x, K.expand_dims(kernel)), axis=-1)\n",
    "    else:\n",
    "        return K.dot(x, kernel)\n",
    "    \n",
    "\n",
    "class AttentionWithContext(Layer):\n",
    "    \"\"\"\n",
    "    Attention operation, with a context/query vector, for temporal data.\n",
    "    Supports Masking.\n",
    "    Follows the work of Yang et al. [https://www.cs.cmu.edu/~diyiy/docs/naacl16.pdf]\n",
    "    \"Hierarchical Attention Networks for Document Classification\"\n",
    "    by using a context vector to assist the attention\n",
    "    # Input shape\n",
    "        3D tensor with shape: `(samples, steps, features)`.\n",
    "    # Output shape\n",
    "        2D tensor with shape: `(samples, features)`.\n",
    "    How to use:\n",
    "    Just put it on top of an RNN Layer (GRU/LSTM/SimpleRNN) with return_sequences=True.\n",
    "    The dimensions are inferred based on the output shape of the RNN.\n",
    "    Note: The layer has been tested with Keras 2.0.6\n",
    "    Example:\n",
    "        model.add(LSTM(64, return_sequences=True))\n",
    "        model.add(AttentionWithContext())\n",
    "        # next add a Dense layer (for classification/regression) or whatever...\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 W_regularizer=None, u_regularizer=None, b_regularizer=None,\n",
    "                 W_constraint=None, u_constraint=None, b_constraint=None,\n",
    "                 bias=True, **kwargs):\n",
    "\n",
    "        self.supports_masking = True\n",
    "        self.init = initializers.get('glorot_uniform')\n",
    "\n",
    "        self.W_regularizer = regularizers.get(W_regularizer)\n",
    "        self.u_regularizer = regularizers.get(u_regularizer)\n",
    "        self.b_regularizer = regularizers.get(b_regularizer)\n",
    "\n",
    "        self.W_constraint = constraints.get(W_constraint)\n",
    "        self.u_constraint = constraints.get(u_constraint)\n",
    "        self.b_constraint = constraints.get(b_constraint)\n",
    "\n",
    "        self.bias = bias\n",
    "        super(AttentionWithContext, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape) == 3\n",
    "\n",
    "        self.W = self.add_weight((input_shape[-1], input_shape[-1],),\n",
    "                                 initializer=self.init,\n",
    "                                 name='{}_W'.format(self.name),\n",
    "                                 regularizer=self.W_regularizer,\n",
    "                                 constraint=self.W_constraint)\n",
    "        if self.bias:\n",
    "            self.b = self.add_weight((input_shape[-1],),\n",
    "                                     initializer='zero',\n",
    "                                     name='{}_b'.format(self.name),\n",
    "                                     regularizer=self.b_regularizer,\n",
    "                                     constraint=self.b_constraint)\n",
    "\n",
    "        self.u = self.add_weight((input_shape[-1],),\n",
    "                                 initializer=self.init,\n",
    "                                 name='{}_u'.format(self.name),\n",
    "                                 regularizer=self.u_regularizer,\n",
    "                                 constraint=self.u_constraint)\n",
    "\n",
    "        super(AttentionWithContext, self).build(input_shape)\n",
    "\n",
    "    def compute_mask(self, input, input_mask=None):\n",
    "        # do not pass the mask to the next layers\n",
    "        return None\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        uit = dot_product(x, self.W)\n",
    "\n",
    "        if self.bias:\n",
    "            uit += self.b\n",
    "\n",
    "        uit = K.tanh(uit)\n",
    "        ait = dot_product(uit, self.u)\n",
    "\n",
    "        #a = K.exp(ait)\n",
    "        a = K.softmax(ait)\n",
    "        # apply mask after the exp. will be re-normalized next\n",
    "        #if mask is not None:\n",
    "        #    # Cast the mask to floatX to avoid float64 upcasting in theano\n",
    "        #    a *= K.cast(mask, K.floatx())\n",
    "\n",
    "        # in some cases especially in the early stages of training the sum may be almost zero\n",
    "        # and this results in NaN's. A workaround is to add a very small positive number ε to the sum.\n",
    "        # a /= K.cast(K.sum(a, axis=1, keepdims=True), K.floatx())\n",
    "        #a /= K.cast(K.sum(a, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n",
    "        a = K.expand_dims(a)\n",
    "        weighted_input = x * a\n",
    "        return K.sum(weighted_input, axis=1)\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape[0], input_shape[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# None is present as the first dimension which represents the number of sentences passed.\n",
    "# 100 words are input \n",
    "sentence_input = Input(shape=(MAX_SENT_LENGTH,), dtype='int32')\n",
    "# These 100 words are converted to (100,100) array using the embedding matrix.\n",
    "embedded_sequences = embedding_layer(sentence_input)\n",
    "# These (100,100) are input to GRU at each time-step (100 time-steps) where each time step takes (1,100) as input and\n",
    "# produces (1,100) as output. As bi-directional GRU is used, the 100 from front is concatenated with 100 from back and\n",
    "# a 200 dimensional vector is output at each timestep.\n",
    "# This occurs for 100 timesteps. so, (100,200) is the output shape of this layer.\n",
    "l_lstm = Bidirectional(GRU(100, return_sequences=True))(embedded_sequences)\n",
    "#l_tan = TimeDistributed(Dense(1,activation='tanh'))(l_lstm)\n",
    "# These 100 timesteps are converted to a single vector using the attention mechanism. So, output is 200 dim vector.\n",
    "l_att = AttentionWithContext()(l_lstm)\n",
    "sentEncoder = Model(sentence_input, l_att)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         (None, 60)                0         \n",
      "_________________________________________________________________\n",
      "embedding_2 (Embedding)      (None, 60, 100)           10027800  \n",
      "_________________________________________________________________\n",
      "spatial_dropout1d_1 (Spatial (None, 60, 100)           0         \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 60, 200)           160800    \n",
      "_________________________________________________________________\n",
      "attention_with_context_1 (At (None, 200)               40400     \n",
      "=================================================================\n",
      "Total params: 10,229,000\n",
      "Trainable params: 10,229,000\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "input_layer = Input(shape=(MAX_SENTS,MAX_SENT_LENGTH),dtype='int32')\n",
    "sentence_input = Input(shape=(MAX_SENT_LENGTH,),dtype='int32')\n",
    "embedding_layer = Embedding(len(word_index) + 1,\n",
    "                            EMBEDDING_DIM,\n",
    "                            weights=[embedding_matrix],\n",
    "                            input_length=MAX_SENT_LENGTH,\n",
    "                            trainable=True,\n",
    "                            mask_zero=True)(sentence_input)\n",
    "drop1 = SpatialDropout1D(0.3)(embedding_layer)\n",
    "sent_lstm = Bidirectional(LSTM(100, name='blstm_1',\n",
    "        activation='tanh',\n",
    "        recurrent_activation='hard_sigmoid',\n",
    "        recurrent_dropout=0.0,\n",
    "        dropout=0.4, \n",
    "        kernel_initializer='glorot_uniform',\n",
    "        return_sequences=True),\n",
    "        merge_mode='concat')(drop1)\n",
    "#sent_lstm_1 = BatchNormalization()(sent_lstm)\n",
    "sent_att_layer = AttentionWithContext()(sent_lstm)\n",
    "sentEncoder = Model(sentence_input, sent_att_layer)\n",
    "sentEncoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sentEncoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The input to the entire network is an entire document which consists of 15 sentences and each sentence have atmost 100 words.\n",
    "# So, an array of size (15,100) is passed as input.\n",
    "review_input = Input(shape=(MAX_SENTS, MAX_SENT_LENGTH), dtype='int32')\n",
    "# Now each sentence needs to go through the sentence encoder to obtain sentence representations. So, a time distributed \n",
    "# layer is used which distributes the same network across time (each sentence).\n",
    "review_encoder = TimeDistributed(sentEncoder)(review_input)\n",
    "# The output of the sentence encoder network is a 200 dim vector. But as we are passing 15 sentence every time we get \n",
    "# a 15,200 dim array as output.\n",
    "# Now this 15,200 is input to a bi-direction GRU. Each time step takes 1,200 as input. Why not double??\n",
    "l_lstm_sent = Bidirectional(GRU(100, return_sequences=True))(review_encoder)\n",
    "# Attention network converts the 15 sentences to a single representation. So, we get just a 200 dim vector.\n",
    "#l_tan_sent = TimeDistributed(Dense(1,activation='tanh'))(l_lstm_sent)\n",
    "l_att_sent = AttentionWithContext()(l_lstm_sent)\n",
    "preds = Dense(2, activation='softmax')(l_att_sent)\n",
    "model = Model(review_input, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 15, 60)            0         \n",
      "_________________________________________________________________\n",
      "time_distributed_1 (TimeDist (None, 15, 200)           10229000  \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 15, 200)           0         \n",
      "_________________________________________________________________\n",
      "bidirectional_2 (Bidirection (None, 15, 200)           240800    \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 15, 200)           800       \n",
      "_________________________________________________________________\n",
      "attention_with_context_2 (At (None, 200)               40400     \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 2)                 402       \n",
      "=================================================================\n",
      "Total params: 10,511,402\n",
      "Trainable params: 10,511,002\n",
      "Non-trainable params: 400\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "textEncoder = TimeDistributed(sentEncoder)(input_layer)\n",
    "drop2 = Dropout(0.4)(textEncoder)\n",
    "\n",
    "lstm_1 = Bidirectional(LSTM(100, name='blstm_2',\n",
    "        activation='tanh',\n",
    "        recurrent_activation='hard_sigmoid',\n",
    "        recurrent_dropout=0.0,\n",
    "        dropout=0.4, \n",
    "        kernel_initializer='glorot_uniform',\n",
    "        return_sequences=True),\n",
    "        merge_mode='concat')(drop2)\n",
    "lstm_1 = BatchNormalization()(lstm_1)\n",
    "att_layer = AttentionWithContext()(lstm_1)\n",
    "drop3 = Dropout(0.5)(att_layer)\n",
    "predictions = Dense(2, activation='softmax')(drop3)\n",
    "model = Model(inputs=input_layer, outputs=predictions)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(\"model fitting - Hierachical attention network\")\n",
    "model.fit(x_train, y_train, validation_data=(x_val, y_val),\n",
    "          nb_epoch=10, batch_size=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get output before the attention layer\n",
    "get_layer_output = K.function([model.layers[0].input, K.learning_phase()], [model.layers[4].output])\n",
    "out = get_layer_output([x_train[0],0])  # test mode\n",
    "print(out[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Get the attention weights of the sentence network.\n",
    "eij = np.tanh(np.dot(out[0],model.layers[5].get_weights()[0]))\n",
    "t = np.dot(eij,model.layers[5].get_weights()[2])\n",
    "ai = np.exp(t)\n",
    "weights = ai/np.sum(ai)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 15 weights for 15 sentences.\n",
    "weights.shape\n",
    "#np.sum(out[0]*weights,axis=1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(15, 200)\n"
     ]
    }
   ],
   "source": [
    "# Get the attention weights of the word network for every sentence.\n",
    "get_layer_op_words = K.function([sentEncoder.layers[0].input, K.learning_phase()], [sentEncoder.layers[4].output])\n",
    "op_words = get_layer_op_words([x_train[0],0])\n",
    "print(op_words[0].shape)                                                                                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-1bba9d06ee12>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mweight_all_words\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMAX_SENTS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0meij_words\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtanh\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop_words\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msentEncoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mt_words\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meij_words\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msentEncoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mai_words\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_words\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "weight_all_words = []\n",
    "for i in range(MAX_SENTS):\n",
    "    eij_words = np.tanh(np.dot(op_words[0][i],sentEncoder.layers[5].get_weights()[0]))\n",
    "    t_words = np.dot(eij_words,sentEncoder.layers[5].get_weights()[2])\n",
    "    ai_words = np.exp(t_words)\n",
    "    weights_words = ai_words/np.sum(ai_words)\n",
    "    weight_all_words.append(weights_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 60 weights for 60 words \n",
    "len(weight_all_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#weight_all_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "id2word = {v: k for k, v in word_index.items()}\n",
    "# Iterate through the top sentences\n",
    "sent_no = 0\n",
    "for i in x_train[0]:\n",
    "    # Iterate through the words in the sentence\n",
    "    weights_words = weight_all_words[sent_no]\n",
    "    sent = ''\n",
    "    word_no =0\n",
    "    for j in i:\n",
    "        if j!=0:\n",
    "            sent+=id2word[j]+' '+str(weights_words[word_no] * weights[0][sent_no])+' ' \n",
    "        word_no+=1\n",
    "    print(str(weights[0][sent_no])+'---'+sent+'.')\n",
    "    sent_no+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(\"visualization.html\", \"w\") as html_file:\n",
    "    html_file.write('<!DOCTYPE html>\\n')\n",
    "    html_file.write('<html>\\n')\n",
    "    html_file.write('<body>\\n')\n",
    "    sent_no = 0\n",
    "    for i in x_train[0]:\n",
    "        # Iterate through the words in the sentence\n",
    "        weights_words = weight_all_words[sent_no]\n",
    "        sent = ''\n",
    "        word_no =0\n",
    "        html_file.write('<font style=\"background-color: rgba(255, 0, 0, %f)\">%s</font>\\n' %(weights[0][sent_no],str(weights[0][sent_no])))\n",
    "        for j in i:\n",
    "            if j!=0:\n",
    "                alpha = weights_words[word_no]*50\n",
    "                word = id2word[j]\n",
    "                html_file.write('<font style=\"background-color: rgba(0, 0, 255, %f)\">%s</font>\\n' % (alpha, word))\n",
    "                sent+=id2word[j]+' '+str(weights_words[word_no])+' ' \n",
    "            word_no+=1\n",
    "        html_file.write('<br />\\n')\n",
    "        print(str(weights[0][sent_no])+'---'+sent+'.')\n",
    "        sent_no+=1\n",
    "    html_file.write('</body>\\n')\n",
    "    html_file.write('</html>\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Blues = plt.get_cmap('Blues')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sentEncoder.layers[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.AttentionWithContext at 0x1a3185ca20>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentEncoder.layers[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (textClassifier)",
   "language": "python",
   "name": "textclassifier"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
