{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import glob\n",
    "from pathlib import Path\n",
    "import unicodedata\n",
    "import re\n",
    "import nltk\n",
    "from nltk import tokenize\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def preprocess(text):\n",
    "    text = text.lower()\n",
    "    doc = word_tokenize(text)\n",
    "    doc = [word for word in doc if word.isalpha()]\n",
    "    return doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('../document similarity/document_similarity_hatt.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('../GDELT_2017/unrest_gdelt_2017_all.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = df.sample(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = df.iloc[1].content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Function for removing unnecessary characters.\n",
    "def clean_str(string):\n",
    "    \"\"\"\n",
    "    Tokenization/string cleaning for dataset\n",
    "    Every dataset is lower cased except\n",
    "    \"\"\"\n",
    "    string = re.sub(r\"\\\\\", \" \", string)\n",
    "    string = re.sub(r\"\\'\", \" \", string)\n",
    "    string = re.sub(r\"\\\"\", \" \", string)\n",
    "    string = re.sub(r\"\\n\", \" \", string)\n",
    "    string = re.sub(r\"=\", \" \", string)\n",
    "    string = re.sub(r\"-\", \" \", string)\n",
    "    string = re.sub(r\"/\", \" \", string)\n",
    "    return string.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_data = unicodedata.normalize(\"NFKD\", data)\n",
    "test_data = clean_str(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from gensim import corpora\n",
    "from gensim.models import TfidfModel\n",
    "from gensim.models import LsiModel\n",
    "from gensim.similarities import MatrixSimilarity\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "word2vec_model = KeyedVectors.load_word2vec_format(\"../GoogleNews-vectors-negative300.bin\", binary=True)\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.metrics.pairwise import pairwise_distances\n",
    "from gensim.similarities import WmdSimilarity\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "from numpy import dot\n",
    "from numpy.linalg import norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Google Sentence Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow_hub as hub\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.logging.set_verbosity(tf.logging.ERROR)\n",
    "embed = hub.Module(\"https://tfhub.dev/google/universal-sentence-encoder/1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def run_gse_benchmark(sentence1, sentence2):\n",
    "    sts_input1 = tf.placeholder(tf.string, shape=(None))\n",
    "    sts_input2 = tf.placeholder(tf.string, shape=(None))\n",
    "\n",
    "    sts_encode1 = tf.nn.l2_normalize(embed(sts_input1))\n",
    "    sts_encode2 = tf.nn.l2_normalize(embed(sts_input2))\n",
    "        \n",
    "    sim_scores = tf.reduce_sum(tf.multiply(sts_encode1, sts_encode2), axis=1)\n",
    "    \n",
    "    with tf.Session() as session:\n",
    "        session.run(tf.global_variables_initializer())\n",
    "        session.run(tf.tables_initializer())\n",
    "      \n",
    "        [gse_sims] = session.run(\n",
    "            [sim_scores],\n",
    "            feed_dict={\n",
    "                sts_input1: sentence1,\n",
    "                sts_input2: sentence2\n",
    "            })\n",
    "    return gse_sims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_data = 'Anna Hazare on fast unto death demanding Jan Lokpal Bill  Veteran Gandhian, Mr Anna Hazare, began a fast unto death in the Capital on Tuesday demanding enactment of a comprehensive Jan Lokpal Bill to check corruption. Ignoring an appeal by the Prime Minister, the 72 year old social activist began the Prime Minister day by paying tribute to Mahatma Gandhi at Rajghat. the Prime Minister was joined by activists such as Swami Agnivesh, former IPS officer and activist Ms Kiran Bedi and Magsaysay award winner Mr Sandeep Pandey. “I will observe fast unto death till the Government agrees to form a joint committee comprising 50 per cent officials and the remaining citizens and intellectuals to draft the Jan Lokpal Bill,” Mr Hazare said at Rajghat. On the way to the fast site at Jantar Mantar, Mr Hazare s open jeep was greeted by tricolour waving supporters, including school children. On Monday night, the Prime Minister s Office had said that the Prime Minister s had enormous respect for Mr Hazare and the Prime Minister s mission. “the Prime Minister s says we trust you (Mr Hazare), we respect you. But, then why did the Prime Minister s not sit with we even once after the meeting last month,” said Mr Hazare. Mr Hazare said Mr Hazare was disappointed after the Prime Minister s had, in the last meeting with social activists over the proposed law to tackle corruption, rejected social activists over the proposed law demand for a joint committee. “If the Government alone drafts this Bill, it will be autocratic not democratic,” Mr Hazare said. Mr Hazare lamented that views of eminent persons such as Justice (Retd) Mr Santosh Hegde, senior lawyer Mr Prashant Bhushan and Swami Agnivesh “were not considered important by the Government alone. Meanwhile, reports said that during ‘Gudi Padwa  celebrations in his village Ralegan Siddhi in Ahmednagar district, Maharashtra, the villagers expressed the villagers support by dressing the villagers ‘gudis  in black clothes as a mark of protest. Activists of the ‘India Against Corruption  movement, too, began a fast at Azad Maidan in Mumbai in support of Mr Hazare and a rally of around 100 cars and bikes was organised from Shivaji Park in Central Mumbai to Azad Maidan.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import gensim\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "glove2word2vec(glove_input_file=\"../glove.840B.300d.txt\", word2vec_output_file=\"../gensim_glove_vectors.txt\")\n",
    "glove_model = KeyedVectors.load_word2vec_format(\"../gensim_glove_vectors.txt\", binary=False)\n",
    "word2vec_model = KeyedVectors.load_word2vec_format(\"../GoogleNews-vectors-negative300.bin\", binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Word2vec and glove averages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from collections import Counter\n",
    "import math\n",
    "import nltk\n",
    "\n",
    "STOP = set(nltk.corpus.stopwords.words(\"english\"))\n",
    "\n",
    "def run_avg_benchmark(sentences1, sentences2, model=None, use_stoplist=False,doc_freqs = None): \n",
    "    \n",
    "    sims = []\n",
    "    for (sent1, sent2) in zip(sentences1, sentences2):\n",
    "    \n",
    "        tokens1 = [t.lower() for t in nltk.word_tokenize(sent1)]\n",
    "        #tokens1 = [t for t in tokens1 if t not in STOP]\n",
    "        \n",
    "        tokens2 = [t.lower() for t in nltk.word_tokenize(sent2)]\n",
    "        #tokens2 = [t for t in tokens2 if t not in STOP]\n",
    "        \n",
    "        if len(tokens1) == 0 or len(tokens2) == 0:\n",
    "            sims.append(0)\n",
    "            continue\n",
    "        \n",
    "        tokfreqs1 = Counter(tokens1)\n",
    "        tokfreqs2 = Counter(tokens2)\n",
    "        \n",
    "        weights1 = [tokfreqs1[token] * math.log(N/(doc_freqs.get(token, 0)+1)) \n",
    "                    for token in tokfreqs1] if doc_freqs else None\n",
    "        weights2 = [tokfreqs2[token] * math.log(N/(doc_freqs.get(token, 0)+1)) \n",
    "                    for token in tokfreqs2] if doc_freqs else None\n",
    "        \n",
    "        emb1 = []\n",
    "        emb2 = []\n",
    "        for token in tokfreqs1:\n",
    "            try:\n",
    "                emb1.append(model[token])\n",
    "            except:\n",
    "                continue;\n",
    "                \n",
    "        for token in tokfreqs2:\n",
    "            try:\n",
    "                emb2.append(model[token])\n",
    "            except:\n",
    "                continue;\n",
    "                \n",
    "        embedding1 = np.average(emb1, axis=0, weights=weights1).reshape(1, -1)\n",
    "        embedding2 = np.average(emb2, axis=0, weights=weights2).reshape(1, -1)\n",
    "\n",
    "        sim = cosine_similarity(embedding1, embedding2)[0][0]\n",
    "        sims.append(sim)\n",
    "\n",
    "    return sims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# WMD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def run_wmd_benchmark(sentences1, sentences2, model, use_stoplist=False):\n",
    "    \n",
    "    sims = []\n",
    "    for (sent1, sent2) in zip(sentences1, sentences2):\n",
    "    \n",
    "        tokens1 = [t.lower() for t in nltk.word_tokenize(sent1)]\n",
    "        #tokens1 = [t for t in tokens1 if t not in STOP]\n",
    "        \n",
    "        tokens2 = [t.lower() for t in nltk.word_tokenize(sent2)]\n",
    "        #tokens2 = [t for t in tokens2 if t not in STOP]\n",
    "        \n",
    "        model.init_sims(replace=True)\n",
    "        sims_wmd = WmdSimilarity([tokens1],model)\n",
    "        sw = sims_wmd[tokens2]\n",
    "        sims.append(sw)\n",
    "        \n",
    "    return sims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#SIF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from numpy import dot\n",
    "from numpy.linalg import norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "def remove_first_principal_component(X):\n",
    "    svd = TruncatedSVD(n_components=1, n_iter=7, random_state=0)\n",
    "    svd.fit(X)\n",
    "    pc = svd.components_\n",
    "    XX = X - X.dot(pc.transpose()) * pc\n",
    "    return XX\n",
    "\n",
    "\n",
    "def run_sif_benchmark(sentences1, sentences2, model, freqs={}, use_stoplist=False, a=0.001): \n",
    "    total_freq = sum(freqs.values())\n",
    "    \n",
    "    embeddings = []\n",
    "    \n",
    "    # SIF requires us to first collect all sentence embeddings and then perform \n",
    "    # common component analysis.\n",
    "    for (sent1, sent2) in zip(sentences1, sentences2): \n",
    "        \n",
    "        tokens1 = [t.lower() for t in nltk.word_tokenize(sent1)]\n",
    "        tokens2 = [t.lower() for t in nltk.word_tokenize(sent2)]\n",
    "        \n",
    "        tokens1 = [token for token in tokens1 if token in model]\n",
    "        tokens2 = [token for token in tokens2 if token in model]\n",
    "        \n",
    "        weights1 = [a/(a+freqs.get(token,0)/total_freq) for token in tokens1]\n",
    "        weights2 = [a/(a+freqs.get(token,0)/total_freq) for token in tokens2]\n",
    "        \n",
    "        embedding1 = np.average([model[token] for token in tokens1], axis=0, weights=weights1)\n",
    "        embedding2 = np.average([model[token] for token in tokens2], axis=0, weights=weights2)\n",
    "        \n",
    "        embeddings.append(embedding1)\n",
    "        embeddings.append(embedding2)\n",
    "        \n",
    "    embeddings = remove_first_principal_component(np.array(embeddings))\n",
    "\n",
    "    cos_sim = dot(embeddings[0], embeddings[1])/(norm(embeddings[0])*norm(embeddings[1]))\n",
    "\n",
    "    return cos_sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Infer sent\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from models import InferSent\n",
    "model_version = 1\n",
    "MODEL_PATH = \"encoder/infersent%s.pkl\" % model_version\n",
    "params_model = {'bsize': 64, 'word_emb_dim': 300, 'enc_lstm_dim': 2048,\n",
    "                'pool_type': 'max', 'dpout_model': 0.0, 'version': model_version}\n",
    "model = InferSent(params_model)\n",
    "model.load_state_dict(torch.load(MODEL_PATH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Keep it on CPU or put it on GPU\n",
    "use_cuda = False\n",
    "model = model.cuda() if use_cuda else model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# If infersent1 -> use GloVe embeddings. If infersent2 -> use InferSent embeddings.\n",
    "W2V_PATH = 'dataset/GloVe/glove.840B.300d.txt' #if model_version == 1 else '../dataset/fastText/crawl-300d-2M.vec'\n",
    "model.set_w2v_path(W2V_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size : 100000\n"
     ]
    }
   ],
   "source": [
    "# Load embeddings of K most frequent words\n",
    "model.build_vocab_k_words(K=100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sentence1 = \"A protest occurred in Nebraska between two groups.\"\n",
    "sentence2 = \"A protest occured between two groups in Lincoln, Nebraska.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cosine(u, v):\n",
    "    return np.dot(u, v) / (np.linalg.norm(u) * np.linalg.norm(v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9106413"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine(model.encode([sentence1],tokenize=True)[0], model.encode([sentence2],tokenize=True)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sims_infer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sims = run_gse_benchmark([sentence1],[sentence2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sims_avg_w2v = run_avg_benchmark([sentence1],[sentence2], word2vec_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sims_avg_glove = run_avg_benchmark([sentence1],[sentence2], glove_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sims_wmd_w2v = run_wmd_benchmark([sentence1],[sentence2], word2vec_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sims_wmd_glove = run_wmd_benchmark([sentence1],[sentence2], glove_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_tsv(f):\n",
    "    frequencies = {}\n",
    "    with open(f) as tsv:\n",
    "        tsv_reader = csv.reader(tsv, delimiter=\"\\t\")\n",
    "        for row in tsv_reader: \n",
    "            frequencies[row[0]] = int(row[1])\n",
    "        \n",
    "    return frequencies\n",
    "        \n",
    "frequencies = read_tsv('frequencies.tsv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#sims_sif_w2v = run_sif_benchmark([\"The dog ate the cat\"],[\"The cat was eaten by a dog\"] ,word2vec_model,freqs=frequencies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#sims_sif_glove = run_sif_benchmark([\"Nebraska\"],[\"United States\"], glove_model,freqs=frequencies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#sims_sif_w2v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "all_sims = []\n",
    "bottom_sims = []\n",
    "for ind,row in df.iterrows():\n",
    "    print(ind)\n",
    "    sim_doc = []\n",
    "    bottom_doc = []\n",
    "    test_data = unicodedata.normalize(\"NFKD\", row['content'])\n",
    "    test_data = clean_str(test_data)\n",
    "    sents = nltk.sent_tokenize(test_data)\n",
    "    title = row['title']\n",
    "    if len(sents)>5:\n",
    "        for s in sents[0:5]:\n",
    "            print(s)\n",
    "            sims = run_gse_benchmark([title],[s])\n",
    "            sim_doc.append(sims[0])\n",
    "        all_sims.append(sim_doc)\n",
    "        for s in sents[-5:]:\n",
    "            print(s)\n",
    "            sims = run_gse_benchmark([title],[s])\n",
    "            bottom_doc.append(sims[0])\n",
    "        bottom_sims.append(bottom_doc)\n",
    "    if (ind+1)%100 == 0:\n",
    "        t_sims = np.array(all_sims)\n",
    "        b_sims = np.array(bottoms_sims)\n",
    "        np.save('top_sim_measures'+str(ind)+'.npy',t_sims)\n",
    "        np.save('bottom_sim_measures'+str(ind)+'.npy',t_sims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('doc_sims.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sentence1 = df.iloc[0].content\n",
    "sentence1 = clean_str(sentence1)\n",
    "sentence2 = df.iloc[1].content\n",
    "sentence2 = clean_str(sentence2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sims = run_gse_benchmark([sentence1],[sentence2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "i=5;\n",
    "while(i<=8):\n",
    "    print(i)\n",
    "    sentence1 = clean_str(df.iloc[i].content)\n",
    "    j=i+1;\n",
    "    while(j<=8):\n",
    "        print(\"----\");\n",
    "        print(j);\n",
    "        sentence2 = clean_str(df.iloc[j].content)\n",
    "        sims = run_gse_benchmark([sentence1],[sentence2])\n",
    "        print(sims)\n",
    "        j=j+1;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sent_nos = {'Ahmednagar': [11],\n",
    " 'Azad Maidan': [12],\n",
    " 'Central Mumbai': [12],\n",
    " 'Jantar Mantar': [4],\n",
    " 'Maharashtra': [11],\n",
    " 'Mahatma Gandhi': [1],\n",
    " 'Monday night': [5],\n",
    " 'Mumbai': [12],\n",
    " 'Rajghat': [1, 3],\n",
    " 'Ralegan Siddhi': [11],\n",
    " 'Shivaji Park': [12]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sent_nos = {'Anna Hazare': {0, 3, 4, 5, 6, 7, 8, 9, 10, 12},\n",
    " 'Jan Lokpal Bill Veteran Gandhian': {0, 3, 9},\n",
    " 'Kiran Bedi': {2},\n",
    " 'Mahatma Gandhi': {1},\n",
    " 'Prashant Bhushan': {10},\n",
    " 'Prime Minister s Office': {5},\n",
    " 'Sandeep Pandey': {2},\n",
    " 'Santosh Hegde': {10},\n",
    " 'Swami Agnivesh': {2, 10},\n",
    " '‘ India Against Corruption': {12}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text_sent = nltk.sent_tokenize(test_data)\n",
    "sent_scores = {k:-1 for k in sent_nos.keys()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "title = 'Anna Hazare on fast unto death demanding Jan Lokpal Bill'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for s in sent_nos.keys():\n",
    "    l = sent_nos[s]\n",
    "    sim = []\n",
    "    for x in l:\n",
    "        sims = run_gse_benchmark([title],[text_sent[x]])\n",
    "        sim.append(sims)\n",
    "    sent_scores[s] = sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(sent_scores['Jan Lokpal Bill Veteran Gandhian'])/len(sent_scores['Jan Lokpal Bill Veteran Gandhian'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
