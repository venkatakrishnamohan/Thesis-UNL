{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from mediawiki import MediaWiki\n",
    "from bs4 import BeautifulSoup\n",
    "import string\n",
    "from nltk import tokenize\n",
    "from nltk.tag import StanfordNERTagger\n",
    "import string\n",
    "from nltk.tokenize import word_tokenize\n",
    "import unicodedata\n",
    "import timeit\n",
    "from fuzzywuzzy import fuzz\n",
    "from fuzzywuzzy import process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Using the Stanfors' default tagger\n",
    "st = StanfordNERTagger('/Users/venkatakrishnamohansunkara/Desktop/DM/stanford-ner-2018-02-27/classifiers/english.all.3class.distsim.crf.ser.gz',\n",
    "  '/Users/venkatakrishnamohansunkara/Desktop/DM/stanford-ner-2018-02-27/stanford-ner.jar',\n",
    "   encoding='utf-8') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Lists containing common prefixes and suffixes.\n",
    "prefix = ['at','by','near','in','from', 'on','to']\n",
    "# https://www.irfca.org/docs/place-names.html\n",
    "suffixes = ['nagar','colony','street','road','hill','river','temple']\n",
    "subs = ['pur','puram','ghar','pura','ganj','abad','halli','keri']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "preps_score={'\"': 0.99,\n",
    " 'down': 0.99,\n",
    " 'across': 0.25,\n",
    " 'near': 0.67,\n",
    " 'inside': 0.23,\n",
    " 'amid': 0.23,\n",
    " 'in': 0.22,\n",
    " 'from': 0.205,\n",
    " 'off': 0.2,\n",
    " 'at': 0.19,\n",
    " 'like': 0.18,\n",
    " 'outside': 0.12,\n",
    " 'because': 0.1,\n",
    " 'of': 0.09,\n",
    " 'about': 0.08,\n",
    " 'between': 0.075,\n",
    " 'by': 0.07,\n",
    " 'among': 0.07,\n",
    " 'with': 0.07,\n",
    " 'against': 0.07,\n",
    " 'into': 0.06,\n",
    " 'that': 0.05,\n",
    " 'through': 0.045,\n",
    " 'during':0.045,\n",
    " 'after': 0.045,\n",
    " 'on': 0.03,\n",
    " 'while':0.03,\n",
    " 'for': 0.02,\n",
    " 'as': 0.015}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vocab = [\"strike\",\n",
    "\"unrest\",\n",
    "\"masses\",\n",
    "\"protest\",\n",
    "\"demonstration\",\n",
    "\"worker\",\n",
    "\"union\",\n",
    "\"company\",\n",
    "\"caste\",\n",
    "\"religious\",\n",
    "\"ethnic\",\n",
    "\"reformed\",\n",
    "\"rebellion\",\n",
    "\"defense\",\n",
    "\"violence\",\n",
    "\"war\",\n",
    "\"armed\",\n",
    "\"fight\",\n",
    "\"Right\",\n",
    "\"free\",\n",
    "\"freedom\",\n",
    "\"liberty\",\n",
    "\"justice\",\n",
    "\"Fair\",\n",
    "\"unfair\",\n",
    "\"unequal\",\n",
    "\"terror\",\n",
    "\"extreme\",\n",
    "\"Bomb\",\n",
    "\"IED\",\n",
    "\"weapon\",\n",
    "\"gun\",\n",
    "\"wmd\",\n",
    "\"threat\",\n",
    "\"suicide\",\n",
    "\"murder\",\n",
    "\"Kill\",\n",
    "\"death\",\n",
    "\"explosive\",\n",
    "\"military\",\n",
    "\"police\",\n",
    "\"elite\",\n",
    "\"government\",\n",
    "\"oppresive\",\n",
    "\"power\",\n",
    "\"regime\",\n",
    "\"fraud\",\n",
    "\"corruption\",\n",
    "\"coup\",\n",
    "\"safety\",\n",
    "\"secure\",\n",
    "\"insecure\",\n",
    "\"protect\",\n",
    "\"enemy\",\n",
    "\"resist\",\n",
    "\"hostage\",\n",
    "\"truce\",\n",
    "\"fire\",\n",
    "\"greed\",\n",
    "\"panic\",\n",
    "\"inflation\",\n",
    "\"Price\",\n",
    "\"Food\",\n",
    "\"Water\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Read the data\n",
    "#news_data = pd.read_csv('/Users/venkatakrishnamohansunkara/Desktop/DM/GDELT_2017/large_dataset/unrest_2017_ie_toi.csv', sep=',')\n",
    "df = pd.read_csv('../document similarity/document_similarity_hatt.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_locs = pd.read_csv('all_locations.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1315065, 6)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_locs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Remove non english characters, everything in ()\n",
    "# Sort the dataframe\n",
    "for i,row in all_locs.iterrows():\n",
    "    aname = row['Area Name']\n",
    "    aname = re.sub(\"[\\(\\[].*?[\\)\\]]\", \"\", str(aname))\n",
    "    all_locs.set_value(i,'Area Name',aname.strip().lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_data = df.iloc[1].content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Function for removing unnecessary characters.\n",
    "def clean_str(string):\n",
    "    \"\"\"\n",
    "    Tokenization/string cleaning for dataset\n",
    "    Every dataset is lower cased except\n",
    "    \"\"\"\n",
    "    string = unicodedata.normalize(\"NFKD\", string)\n",
    "    string = re.sub(r\"\\\\\", \" \", string)\n",
    "    string = re.sub(r\"\\'\", \" \", string)\n",
    "    string = re.sub(r\"\\\"\", \" \", string)\n",
    "    string = re.sub(r\"\\n\", \" \", string)\n",
    "    string = re.sub(r\"=\", \" \", string)\n",
    "    string = re.sub(r\"-\", \" \", string)\n",
    "    string = re.sub(r\"/\", \" \", string)\n",
    "    return string.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text_sent = nltk.sent_tokenize(test_data)\n",
    "text_tags = nltk.word_tokenize(text_sent[0])\n",
    "text_tagged = nltk.pos_tag(text_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Anna', 'NNP'),\n",
       " ('Hazare', 'NNP'),\n",
       " ('on', 'IN'),\n",
       " ('fast-unto-death', 'JJ'),\n",
       " ('demanding', 'VBG'),\n",
       " ('Jan', 'NNP'),\n",
       " ('Lokpal', 'NNP'),\n",
       " ('Bill', 'NNP'),\n",
       " ('Veteran', 'NNP'),\n",
       " ('Gandhian', 'NNP'),\n",
       " (',', ','),\n",
       " ('Mr', 'NNP'),\n",
       " ('Anna', 'NNP'),\n",
       " ('Hazare', 'NNP'),\n",
       " (',', ','),\n",
       " ('began', 'VBD'),\n",
       " ('a', 'DT'),\n",
       " ('fast-unto-death', 'NN'),\n",
       " ('in', 'IN'),\n",
       " ('the', 'DT'),\n",
       " ('Capital', 'NN'),\n",
       " ('on', 'IN'),\n",
       " ('Tuesday', 'NNP'),\n",
       " ('demanding', 'VBG'),\n",
       " ('enactment', 'NN'),\n",
       " ('of', 'IN'),\n",
       " ('a', 'DT'),\n",
       " ('comprehensive', 'JJ'),\n",
       " ('Jan', 'NNP'),\n",
       " ('Lokpal', 'NNP'),\n",
       " ('Bill', 'NNP'),\n",
       " ('to', 'TO'),\n",
       " ('check', 'VB'),\n",
       " ('corruption', 'NN'),\n",
       " ('.', '.')]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_tagged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_locations(test_data):\n",
    "    # Convert the entire document data to sentences.\n",
    "    text_sent = nltk.sent_tokenize(test_data)\n",
    "    stanford = []\n",
    "    noun_phrases = []\n",
    "    #new_model = []\n",
    "    locations = []\n",
    "    # other are prefixes\n",
    "    other = []\n",
    "    suffix_locations = []\n",
    "    sub_locations = []\n",
    "    all_words = []\n",
    "    # Iterate over all the sentences.\n",
    "    for sent in text_sent:\n",
    "        # Convert the sentence to words\n",
    "        text_tags = nltk.word_tokenize(sent)\n",
    "        all_words+=[text_tags]\n",
    "        # Get the NER for the words.\n",
    "        original_tags = st.tag(text_tags)\n",
    "        l = len(original_tags)\n",
    "        i=0;\n",
    "        # Iterate over the tagged words.\n",
    "        while i<l:\n",
    "            #print(i)\n",
    "            e,t = original_tags[i];\n",
    "            # If it's a location, then check the next 3 words.\n",
    "            if t == 'LOCATION':\n",
    "                j = 1;\n",
    "                s = e; \n",
    "                # Verify the tags for the next 3 words.\n",
    "                while i+j<len(original_tags):\n",
    "                    # If the next words are also locations, then concatenate them to make a large string.\n",
    "                    if original_tags[i+j][1] == 'LOCATION':\n",
    "                        s = s+\" \"+original_tags[i+j][0];\n",
    "                        j+=1;\n",
    "                    else:\n",
    "                        break;\n",
    "                i = i+j;\n",
    "                # Save the locations to a locations list\n",
    "                locations+=[s];\n",
    "            else:\n",
    "                i=i+1;\n",
    "        # Get the POS tag for the words.\n",
    "        text_tagged = nltk.pos_tag(text_tags)\n",
    "        l = len(text_tagged)\n",
    "        i=0;\n",
    "        # iterate over the tagged words.\n",
    "        while i<l:\n",
    "            #print(i)\n",
    "            e,t = text_tagged[i];\n",
    "            # If the current word is a Noun phrase.\n",
    "            if t == 'NNP':\n",
    "                j = 1;\n",
    "                s = e;\n",
    "                # verify the tags for the next 3 words.\n",
    "                while i+j<len(text_tagged):\n",
    "                    # If the next words are also nouns, then concatenate them to make a large string.\n",
    "                    if text_tagged[i+j][1] in ['NNP','NN','NNS','NNPS']:\n",
    "                        s = s+\" \"+text_tagged[i+j][0];\n",
    "                        j+=1;\n",
    "                    else:\n",
    "                        break;\n",
    "                i = i+j;\n",
    "                # Save the noun phrases to a noun_phrases list\n",
    "                noun_phrases+=[s];\n",
    "            else:\n",
    "                i=i+1;\n",
    "        # Convert the words to lower case so as to compare with the prefixes.\n",
    "        text_tags = [t.lower() for t in text_tags]\n",
    "        # Go through all the prefixes.\n",
    "        for k in prefix:\n",
    "            # If the prefix is not present in the sentence then an error occurs. So, we use a try, catch block\n",
    "            try:\n",
    "                # Get the position the prefix word in a sentence.\n",
    "                index = text_tags.index(k)\n",
    "                # If the word after the prefix word is a noun, then consider it.\n",
    "                if text_tagged[index+1][1] == 'NNP':\n",
    "                    j = 1;\n",
    "                    s = text_tagged[index+1][0]; \n",
    "                    # Verify if it is a phrase by considering the next 3 words.\n",
    "                    while index+j<len(text_tagged):\n",
    "                        if text_tagged[index+j+1][1] in ['NNP','NN','NNS','NNPS']:\n",
    "                            s = s+\" \"+text_tagged[index+j+1][0];\n",
    "                            j+=1;\n",
    "                        else:\n",
    "                            break;\n",
    "                    # Save the words after the prefix words in to other list.\n",
    "                    other = other + [s]\n",
    "            except:\n",
    "                continue\n",
    "        # Iterate through the list of suffixes.\n",
    "        for s in suffixes:\n",
    "            try:\n",
    "                # get the position of the suffix word in the sentence\n",
    "                index = text_tags.index(s)\n",
    "                # If the word before a suffix word is a noun phrase, then consider it.\n",
    "                if text_tagged[index-1][1] == 'NNP':\n",
    "                    j = 1;\n",
    "                    stri = text_tagged[index-1][0]; \n",
    "                    # verify if the preceeding 3 words are nouns.\n",
    "                    while index+j<len(text_tagged):\n",
    "                        if text_tagged[index-j-1][1] in ['NNP','NN','NNS','NNPS']:\n",
    "                            stri = stri+\" \"+text_tagged[index-j-1][0];\n",
    "                            j+=1;\n",
    "                        else:\n",
    "                            break;\n",
    "                    # Reverse the preceeding words list\n",
    "                    stri = ' '.join(stri.split(\" \")[-1::-1]) \n",
    "                    # If the string is not already present in other lists, then add it to the suffix_locations list.\n",
    "                    if stri not in locations or stri not in noun_phrases or stri not in other:\n",
    "                        stri = stri+\" \"+ s;\n",
    "                        if stri not in locations or stri not in noun_phrases or stri not in other:\n",
    "                            suffix_locations = suffix_locations + [stri]\n",
    "            except:\n",
    "                continue\n",
    "        for sub in subs:\n",
    "            # get the words which contain any of the sub_string from the subs list.\n",
    "            su = [te for te in text_tags if te.endswith(sub)]\n",
    "            try:\n",
    "                for sub1 in su:\n",
    "                    # Get the position of the word from the sentence.\n",
    "                    index = text_tags.index(sub1)\n",
    "                    # If it is a Noun, then consider it.\n",
    "                    if text_tagged[index][1] in ['NNP','NN','NNS','NNPS']:\n",
    "                        sub_locations = sub_locations + [text_tagged[index][0]]\n",
    "            except:\n",
    "                continue\n",
    "    all_words = [x for sublist in all_words for x in sublist]\n",
    "    all_locations = locations+other+suffix_locations+sub_locations\n",
    "    all_locations_nodups = []\n",
    "    for i in all_locations:\n",
    "        if i.lower() not in [j.lower() for j in all_locations_nodups]:\n",
    "            all_locations_nodups.append(i);\n",
    "    #Locations: Stanford\n",
    "    #Other: Prepositions\n",
    "    #suffix_locations: suffixes\n",
    "    #sub_locations: sub_strings\n",
    "    return locations,other,suffix_locations,sub_locations,all_locations_nodups, all_words, text_sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_location_sources(locations,other,suffix_locations,sub_locations,all_locations_nodups):\n",
    "    location_sources = {}\n",
    "    for loc in all_locations_nodups:\n",
    "        a = []\n",
    "        if loc in locations:\n",
    "            a.append('stanford')\n",
    "        if loc in other:\n",
    "            a.append('preps')\n",
    "        if loc in suffix_locations:\n",
    "            a.append('suffixes')\n",
    "        if loc in sub_locations:\n",
    "            a.append('sub_strings')\n",
    "        if loc not in location_sources:\n",
    "            location_sources[loc] = a\n",
    "    return location_sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def linear_decay(i,n):\n",
    "    return 1-i/n\n",
    "def exp_decay(i,n):\n",
    "    return np.exp(-(i/n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#get unrest and position occurences of all locations.\n",
    "def get_unrest_words_pos(all_words,locations_list,decay):\n",
    "    pos_threshold = len(all_words)\n",
    "    all_locations = {k:[] for k in locations_list}\n",
    "    nearest_locations = {k:[] for k in locations_list}\n",
    "    pos_occurences = {k:[] for k in locations_list}\n",
    "    pos_scores = {k:-1 for k in pos_occurences.keys()}\n",
    "    unrest_scores = {k:-1 for k in all_locations.keys()}\n",
    "    for location in list(all_locations.keys()):\n",
    "        sub_locations = location.split(' ')\n",
    "        if len(sub_locations) > 1:\n",
    "            last_subloc = sub_locations[-1]\n",
    "            first_subloc = sub_locations[0]\n",
    "            first_indices = [i for i, w in enumerate(all_words) if w == first_subloc]\n",
    "            last_indices = [i for i, w in enumerate(all_words) if w == last_subloc]\n",
    "            diff = [a_i - b_i for a_i, b_i in zip(last_indices, first_indices)]\n",
    "            first_indices = first_indices[diff == len(sub_locations)]\n",
    "            if not isinstance(first_indices,list):\n",
    "                pos_occurences[location].append(first_indices)\n",
    "            else:\n",
    "                pos_occurences[location] = first_indices\n",
    "            indices = pos_occurences[location]\n",
    "        else:\n",
    "            first_indices = [i for i, w in enumerate(all_words) if w == sub_locations[0]]\n",
    "            if not isinstance(first_indices,list):\n",
    "                pos_occurences[location].append(first_indices)\n",
    "            else:\n",
    "                pos_occurences[location] = first_indices\n",
    "            indices = pos_occurences[location]\n",
    "        if len(indices)>0:\n",
    "            no_of_indices = len(indices)\n",
    "            i=0\n",
    "            b=[]\n",
    "            nearest_locs = []\n",
    "            while i< no_of_indices:\n",
    "                j=0\n",
    "                a = []\n",
    "                l = []\n",
    "                while j<pos_threshold and indices[i]+j<len(all_words):\n",
    "                    if all_words[indices[i]+j].lower() in vocab:\n",
    "                        s = [j,indices[i],'r',all_words[indices[i]+j].lower()]\n",
    "                        a.append(s)\n",
    "                    if all_words[indices[i]+j] in locations_list:\n",
    "                        lo = [j,indices[i],'r',all_words[indices[i]+j].lower()]\n",
    "                        l.append(lo)\n",
    "                    j=j+1\n",
    "                j=0\n",
    "                while j<pos_threshold and indices[i]-j>=0:\n",
    "                    if all_words[indices[i]-j].lower() in vocab:\n",
    "                        s = [j,indices[i],'l',all_words[indices[i]-j].lower()]\n",
    "                        a.append(s)\n",
    "                    if all_words[indices[i]-j] in locations_list:\n",
    "                        lo = [j,indices[i],'l',all_words[indices[i]-j].lower()]\n",
    "                        l.append(lo)\n",
    "                    j=j+1\n",
    "                i=i+1\n",
    "                b = b+a\n",
    "                nearest_locs = nearest_locs+l\n",
    "            all_locations[location] = b\n",
    "            nearest_locations[location] = nearest_locs\n",
    "    for i in pos_scores.keys():\n",
    "        a = []\n",
    "        for j in pos_occurences[i]:\n",
    "            if decay == 'linear':\n",
    "                a.append(linear_decay(j,len(all_words)))\n",
    "            else:\n",
    "                a.append(exp_decay(j,len(all_words)))\n",
    "        pos_scores[i] = a\n",
    "    for i in unrest_scores.keys():\n",
    "        a = []\n",
    "        for j in all_locations[i]:\n",
    "            if decay == 'linear':\n",
    "                a.append(linear_decay(j[0],len(all_words)))\n",
    "            else:\n",
    "                a.append(exp_decay(j[0],len(all_words)))\n",
    "        unrest_scores[i] = a\n",
    "    #all_locations: all unrest keywords from all mentions of a location\n",
    "    #nearest location: all other locations from all mentions of a location\n",
    "    #pos_occurences: The positions of locations\n",
    "    #pos_scores, unrest_scores: Scores for a location mentions based on their positon and the unrest keywords\n",
    "    return all_locations,nearest_locations,pos_occurences,pos_scores,unrest_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_unrest_words_sent(sentence_no,text_sent,vocab,decay):\n",
    "    unrest_sent = {k:[] for k in sentence_no.keys()}\n",
    "    other_locs_sent = {k:[] for k in sentence_no.keys()}\n",
    "    lower_keys = [k.lower() for k in sentence_no.keys()]\n",
    "    for i in sentence_no.keys():\n",
    "        sents = sentence_no[i]\n",
    "        a=[]\n",
    "        b=[]\n",
    "        for j in sents:\n",
    "            words = nltk.word_tokenize(text_sent[j].lower())\n",
    "            unrest_words_in_sent = list(set(words) & set(vocab))\n",
    "            other_locs_in_sent = list(set(words)&set(lower_keys))\n",
    "            if len(unrest_words_in_sent) != 0:\n",
    "                s = [i,j,unrest_words_in_sent]\n",
    "                a.append(s)\n",
    "            if len(other_locs_in_sent) != 0:\n",
    "                s1 = [i,j,other_locs_in_sent]\n",
    "                b.append(s1)\n",
    "        unrest_sent[i] = a\n",
    "        other_locs_sent = b\n",
    "    #unrest_sent: all unrest keywords with in the same sentence as a location mention\n",
    "    #other_locs_sent: all other location mentions in the same sentence as a location mention.\n",
    "    return unrest_sent,other_locs_sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_unrest_density_score(unrest_locs,unrest_sent,thresh=None):\n",
    "    unrest_scores = {k:-1 for k in unrest_locs.keys()}\n",
    "    for l in unrest_sent.keys():\n",
    "        score =0;\n",
    "        if len(unrest_sent[l]) >0:\n",
    "            sent_unrest = unrest_sent[l][0][2]\n",
    "            locs_unrest = unrest_locs[l]\n",
    "            for s in sent_unrest:\n",
    "                for l2 in locs_unrest:\n",
    "                    if s == l2[3]:\n",
    "                        score = score+(1/l2[0]);\n",
    "                    else:\n",
    "                        if thresh!=None:\n",
    "                            if l2[0]<thresh:\n",
    "                                score = score+np.exp(-l2[0]);  \n",
    "                        else:\n",
    "                            score = score+np.exp(-l2[0]); \n",
    "        else:\n",
    "            for l2 in unrest_locs[l]:\n",
    "                if thresh!=None:\n",
    "                    if l2[0]<thresh:\n",
    "                        score = score+np.exp(-l2[0]);  \n",
    "                else:\n",
    "                    score = score+np.exp(-l2[0]); \n",
    "        unrest_scores[l] = score/len(unrest_locs[l]);\n",
    "    return unrest_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_frequecies(pos_occurences,all_locations):\n",
    "    locations_freq = {i:0 for i in all_locations}\n",
    "    for i in pos_occurences.keys():\n",
    "        locations_freq[i] = len(pos_occurences[i])\n",
    "    #locatoins_Freq: Number of occurences of each location in a document.\n",
    "    return locations_freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_sent_nos(locations_list,text_sent,decay):\n",
    "    sentence_no = {k:[] for k in locations_list}\n",
    "    sentence_scores = {k:-1 for k in locations_list}\n",
    "    for loc in locations_list:\n",
    "        sub_locations = loc.split(' ')\n",
    "        sent_no = 0\n",
    "        for sent in text_sent:\n",
    "            text_tags = nltk.word_tokenize(sent)\n",
    "            i=0\n",
    "            for s in sub_locations:\n",
    "                if s in text_tags:\n",
    "                    i+=1\n",
    "            if i == len(sub_locations):\n",
    "                sentence_no[loc] = sentence_no[loc]+[sent_no]\n",
    "            sent_no+=1\n",
    "    for loc in locations_list:\n",
    "        a=[]\n",
    "        for i in sentence_no[loc]:\n",
    "            if decay == 'linear':\n",
    "                a.append(linear_decay(i,len(text_sent)))\n",
    "            else:\n",
    "                a.append(exp_decay(i,len(text_sent)))\n",
    "        sentence_scores[loc] = a\n",
    "    # sentence_no: The sentence numbers where a location is mentioned in an article.\n",
    "    return sentence_no,sentence_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_gazette_score(all_locations,all_locs):\n",
    "    gazette_score = {k:-1 for k in all_locations}\n",
    "    for name in all_locations:\n",
    "        def get_ratio(row):\n",
    "            aname = row['Area Name']\n",
    "            return fuzz.ratio(aname,name.lower())\n",
    "        # Extract the closest match > 98%.\n",
    "        # If more than one extract all \n",
    "        found_locs = all_locs[all_locs.apply(get_ratio, axis=1) > 85]\n",
    "        if found_locs.shape[0]>0:\n",
    "            gazette_score[name] = 1\n",
    "        else:\n",
    "            gazette_score[name] = 0\n",
    "    # gazette_Score: score which verifies whether a location is present in the list of gazette locations.\n",
    "    return gazette_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_title_score(all_locations, title):\n",
    "    title_score = {k:-1 for k in all_locations}\n",
    "    for i in all_locations:\n",
    "        if i in title:\n",
    "            title_score[i]=1\n",
    "        else:\n",
    "            title_score[i]=0\n",
    "    #title_score: score which verifies whether a location is mentioned in the title or not.\n",
    "    return title_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_preps_score(all_locations,all_words,preps_score,pos_scores):\n",
    "    preps_scores = {k:[] for k in all_locations}\n",
    "    for loc in all_locations:\n",
    "        sub_locations = loc.split(' ')\n",
    "        indices = pos_scores[loc]\n",
    "        s = []\n",
    "        for i in indices:\n",
    "            try:\n",
    "                if i-1>0 and all_words[i-1] in preps_score.keys():\n",
    "                    s.append(preps_score[all_words[i-1]])\n",
    "                else:\n",
    "                    s.append(0);   \n",
    "            except:\n",
    "                    continue\n",
    "        preps_scores[loc] = s\n",
    "    # preps_scores: Scores to each location mention in an article based on the prepositions preceding them.\n",
    "    return preps_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_final_scores(all_locations,unrest_score,preps_score,title_score,sent_score,position_score,gazette_score,c=6):\n",
    "    final_scores = {k:-1 for k in all_locations}\n",
    "    if c == 6:\n",
    "        for i in all_locations:\n",
    "            score = unrest_score[i]+max(pos_scores[i])+max(preps_score[i])+max(sent_score[i])+title_score[i]+gazette_score[i]\n",
    "            final_scores[i] = score/6;\n",
    "    else:\n",
    "        for i in all_locations:\n",
    "            score = max(pos_scores[i])+max(preps_score[i])+max(sent_score[i])+title_score[i]+gazette_score[i]\n",
    "            final_scores[i] = score/5;\n",
    "    # final_scores: A combination of all the above scores for a location.\n",
    "    return final_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_data = clean_str(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "locations,other,suffix_locations,sub_locations,all_locations,all_words,text_sents = get_locations(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "location_sources = get_location_sources(locations,other,suffix_locations,sub_locations,all_locations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "unrest_locs,nearest_locations,pos_locs,pos_scores,unrest_scores = get_unrest_words_pos(all_words,all_locations,'linear')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "freqs = get_frequecies(pos_locs,all_locations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sent_nos,sent_scores = get_sent_nos(all_locations,text_sents,'linear')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "unrest_sent,other_locs_sent = get_unrest_words_sent(sent_nos,text_sents,vocab,'linear')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "title_scores = get_title_score(all_locations,text_sents[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "preps_scores = get_preps_score(all_locations,all_words,preps_score,pos_locs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gazette_scores = get_gazette_score(all_locations,all_locs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "unrest_scores = get_unrest_density_score(unrest_locs,unrest_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "final_scores = get_final_scores(all_locations,unrest_scores,preps_scores,title_scores,sent_scores,pos_scores,gazette_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "unrest_scores = {k:-1 for k in unrest_locs.keys()}\n",
    "for l in unrest_locs.keys():\n",
    "    a = unrest_locs[l]\n",
    "    mini = math.inf;\n",
    "    for x in a:\n",
    "        if x[0]<mini:\n",
    "            mini = x[0]\n",
    "    unrest_scores[l] = 1 - mini/len(all_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "unrest_sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sent_scores = {'Ahmednagar': [0.528],\n",
    " 'Azad Maidan': [0.617],\n",
    " 'Central Mumbai': [0.617],\n",
    " 'Jantar Mantar': [0.526],\n",
    " 'Maharashtra': [0.528],\n",
    " 'Mahatma Gandhi': [0.598],\n",
    " 'Monday night': [0.627],\n",
    " 'Mumbai': [0.617],\n",
    " 'Rajghat': [0.598, 0.669],\n",
    " 'Ralegan Siddhi': [0.528],\n",
    " 'Shivaji Park': [0.617]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Ahmednagar': 0.4789380234505862,\n",
       " 'Azad Maidan': 0.3099606365159129,\n",
       " 'Central Mumbai': 0.2948601340033501,\n",
       " 'Jantar Mantar': 0.37728978224455617,\n",
       " 'Maharashtra': 0.44101507537688445,\n",
       " 'Mahatma Gandhi': 0.3982428810720268,\n",
       " 'Monday night': 0.34570603015075374,\n",
       " 'Mumbai': 0.4791147403685092,\n",
       " 'Rajghat': 0.6058969849246231,\n",
       " 'Ralegan Siddhi': 0.44478391959798996,\n",
       " 'Shivaji Park': 0.46153936348408714}"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "count={}\n",
    "for u in unrest_sent.keys():\n",
    "    c = unrest_sent[u]\n",
    "    if len(c) == 0:\n",
    "        try:\n",
    "           count[0]+=1;\n",
    "        except:\n",
    "            count[0] = 1;\n",
    "    for s in c:\n",
    "        l = s[2];\n",
    "        try:\n",
    "           count[len(l)]+=1;\n",
    "        except:\n",
    "            count[len(l)] = 1;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 4, 1: 7, 2: 1}"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>countValue</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       countValue\n",
       "count            \n",
       "0               8\n",
       "1              14\n",
       "2               2"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = pd.DataFrame(list(count.items()), columns=['count', 'countValue'])\n",
    "x.append(x).groupby('count').sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Ahmednagar': 0.09877792317907393,\n",
       " 'Azad Maidan': 0.07744945411697447,\n",
       " 'Central Mumbai': 0.05866915620718632,\n",
       " 'Jantar Mantar': 0.2550914830563072,\n",
       " 'Maharashtra': 0.06205114646716531,\n",
       " 'Mahatma Gandhi': 0.295387192372117,\n",
       " 'Monday night': 0.2022290941888932,\n",
       " 'Mumbai': 0.08303736749016066,\n",
       " 'Rajghat': 0.3592430707653918,\n",
       " 'Ralegan Siddhi': 0.06245846483239976,\n",
       " 'Shivaji Park': 0.05795877303251321,\n",
       " 'tricolour': 0.2304851178971782}"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Ahmednagar': ['stanford'],\n",
       " 'Azad Maidan': ['stanford', 'preps'],\n",
       " 'Central Mumbai': ['stanford'],\n",
       " 'Jantar Mantar': ['stanford', 'preps'],\n",
       " 'Maharashtra': ['stanford'],\n",
       " 'Mahatma Gandhi': ['preps'],\n",
       " 'Monday night': ['preps'],\n",
       " 'Mumbai': ['stanford', 'preps'],\n",
       " 'Rajghat': ['preps'],\n",
       " 'Ralegan Siddhi': ['stanford'],\n",
       " 'Shivaji Park': ['stanford', 'preps'],\n",
       " 'tricolour': ['sub_strings']}"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "location_sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Anna Hazare on fast unto death demanding Jan Lokpal Bill  Veteran Gandhian, Mr Anna Hazare, began a fast unto death in the Capital on Tuesday demanding enactment of a comprehensive Jan Lokpal Bill to check corruption. Ignoring an appeal by the Prime Minister, the 72 year old social activist began his day by paying tribute to Mahatma Gandhi at Rajghat. He was joined by activists such as Swami Agnivesh, former IPS officer and activist Ms Kiran Bedi and Magsaysay award winner Mr Sandeep Pandey. “I will observe fast unto death till the Government agrees to form a joint committee comprising 50 per cent officials and the remaining citizens and intellectuals to draft the Jan Lokpal Bill,” Mr Hazare said at Rajghat. On the way to the fast site at Jantar Mantar, Mr Hazare s open jeep was greeted by tricolour waving supporters, including school children. On Monday night, the Prime Minister s Office had said that the Prime Minister had enormous respect for Mr Hazare and his mission. “The Prime Minister says we trust you (Hazare), we respect you. But, then why did he not sit with us even once after the meeting last month,” said Mr Hazare. He said he was disappointed after the Prime Minister had, in the last meeting with social activists over the proposed law to tackle corruption, rejected their demand for a joint committee. “If the Government alone drafts this Bill, it will be autocratic not democratic,” Mr Hazare said. He lamented that views of eminent persons such as Justice (Retd) Mr Santosh Hegde, senior lawyer Mr Prashant Bhushan and Swami Agnivesh “were not considered important by the government“. Meanwhile, reports said that during ‘Gudi Padwa  celebrations in his village Ralegan Siddhi in Ahmednagar district, Maharashtra, the villagers expressed their support by dressing their ‘gudis  in black clothes as a mark of protest. Activists of the ‘India Against Corruption  movement, too, began a fast at Azad Maidan in Mumbai in support of Mr Hazare and a rally of around 100 cars and bikes was organised from Shivaji Park in Central Mumbai to Azad Maidan.'"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a.append(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 2]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'Assam' in list(all_locs['Area Name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
