{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/envs/textClassifier/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(17575, 7)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/envs/textClassifier/lib/python3.6/site-packages/ipykernel_launcher.py:39: FutureWarning: set_value is deprecated and will be removed in a future release. Please use .at[] or .iat[] accessors instead\n",
      "/anaconda3/envs/textClassifier/lib/python3.6/site-packages/ipykernel_launcher.py:41: FutureWarning: set_value is deprecated and will be removed in a future release. Please use .at[] or .iat[] accessors instead\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "#import cPickle\n",
    "from collections import defaultdict\n",
    "import re\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "import sys\n",
    "import os\n",
    "\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer, text_to_word_sequence\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils.np_utils import to_categorical\n",
    "\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Dense, Input, Flatten\n",
    "from keras.layers import Conv1D, MaxPooling1D, Embedding, Dropout, LSTM, GRU, Bidirectional, TimeDistributed, SpatialDropout1D\n",
    "from keras.models import Model\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "\n",
    "from keras import backend as K\n",
    "from keras.engine.topology import Layer, InputSpec\n",
    "from keras import initializers, regularizers, constraints\n",
    "\n",
    "MAX_NB_WORDS = 80000\n",
    "MAX_SEQUENCE_LENGTH = 400\n",
    "VALIDATION_SPLIT = 0.2\n",
    "EMBEDDING_DIM = 100\n",
    "\n",
    "data_train = pd.read_csv('/Users/venkatakrishnamohansunkara/Desktop/DM/classification_data.csv', sep=',')\n",
    "print(data_train.shape)\n",
    "data_train.dropna(inplace=True)\n",
    "\n",
    "data_train['cat_id'] = 0\n",
    "for i,row in data_train.iterrows():\n",
    "    if row['Category'] == 'unrest':\n",
    "        data_train.set_value(i,'cat_id',0)\n",
    "    else:\n",
    "        data_train.set_value(i,'cat_id',1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of positive and negative reviews in traing and validation set\n",
      "[6862. 7112.]\n",
      "[1713. 1780.]\n"
     ]
    }
   ],
   "source": [
    "X_data = data_train['Content']\n",
    "y_data = data_train['cat_id']\n",
    "X_data = list(X_data)\n",
    "y_data = list(y_data)\n",
    "tokenizer = Tokenizer(num_words=MAX_NB_WORDS)\n",
    "tokenizer.fit_on_texts(X_data)\n",
    "X_data = tokenizer.texts_to_sequences(X_data)\n",
    "#X_data = str(X_data)\n",
    "X_data = pad_sequences(X_data,\n",
    "        maxlen=MAX_SEQUENCE_LENGTH,\n",
    "        padding='post',\n",
    "        truncating='post',\n",
    "        dtype='float32')\n",
    "\n",
    "y_data = np.asarray(y_data)\n",
    "y_data = to_categorical(np.asarray(y_data))\n",
    "indices = np.arange(X_data.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "X_data = X_data[indices]\n",
    "y_data = y_data[indices]\n",
    "nb_validation_samples = int(VALIDATION_SPLIT * X_data.shape[0])\n",
    "\n",
    "x_train = X_data[:-nb_validation_samples]\n",
    "y_train = y_data[:-nb_validation_samples]\n",
    "x_val = X_data[-nb_validation_samples:]\n",
    "y_val = y_data[-nb_validation_samples:]\n",
    "\n",
    "print('Number of positive and negative reviews in traing and validation set')\n",
    "print(y_train.sum(axis=0))\n",
    "print(y_val.sum(axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Code for generating input to a test document\n",
    "data_test_df = pd.read_csv('document similarity/document_similarity.csv',sep=',')\n",
    "data_test_content = data_test_df['content']\n",
    "X_data_test = tokenizer.texts_to_sequences(data_test_content)\n",
    "X_data_test = pad_sequences(X_data_test,\n",
    "        maxlen=MAX_SEQUENCE_LENGTH,\n",
    "        padding='post',\n",
    "        truncating='post',\n",
    "        dtype='float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "GLOVE_DIR = \"/Users/venkatakrishnamohansunkara/Desktop/DM/textClassifier\"\n",
    "embeddings_index = {}\n",
    "f = open(os.path.join(GLOVE_DIR, 'glove.6B.100d.txt'))\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "print('Total %s word vectors.' % len(embeddings_index))\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "# Embedding matrix should consist of all the unique words (which is the lenght of word index) as rows and their embeddings as columns (100 dimensions)\n",
    "embedding_matrix = np.random.random((len(word_index) + 1, EMBEDDING_DIM))\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "         #words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "    else:\n",
    "        embedding_matrix[i] = EMBEDDING_DIM*[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def dot_product(x, kernel):\n",
    "    \"\"\"\n",
    "    Wrapper for dot product operation, in order to be compatible with both\n",
    "    Theano and Tensorflow\n",
    "    Args:\n",
    "        x (): input\n",
    "        kernel (): weights\n",
    "    Returns:\n",
    "    \"\"\"\n",
    "    if K.backend() == 'tensorflow':\n",
    "        return K.squeeze(K.dot(x, K.expand_dims(kernel)), axis=-1)\n",
    "    else:\n",
    "        return K.dot(x, kernel)\n",
    "\n",
    "    \n",
    "class AttentionWithContext(Layer):\n",
    "    \"\"\"\n",
    "    Attention operation, with a context/query vector, for temporal data.\n",
    "    Supports Masking.\n",
    "    Follows the work of Yang et al. [https://www.cs.cmu.edu/~diyiy/docs/naacl16.pdf]\n",
    "    \"Hierarchical Attention Networks for Document Classification\"\n",
    "    by using a context vector to assist the attention\n",
    "    # Input shape\n",
    "        3D tensor with shape: `(samples, steps, features)`.\n",
    "    # Output shape\n",
    "        2D tensor with shape: `(samples, features)`.\n",
    "    How to use:\n",
    "    Just put it on top of an RNN Layer (GRU/LSTM/SimpleRNN) with return_sequences=True.\n",
    "    The dimensions are inferred based on the output shape of the RNN.\n",
    "    Note: The layer has been tested with Keras 2.0.6\n",
    "    Example:\n",
    "        model.add(LSTM(64, return_sequences=True))\n",
    "        model.add(AttentionWithContext())\n",
    "        # next add a Dense layer (for classification/regression) or whatever...\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 W_regularizer=None, u_regularizer=None, b_regularizer=None,\n",
    "                 W_constraint=None, u_constraint=None, b_constraint=None,\n",
    "                 bias=True, **kwargs):\n",
    "\n",
    "        self.supports_masking = True\n",
    "        self.init = initializers.get('glorot_uniform')\n",
    "\n",
    "        self.W_regularizer = regularizers.get(W_regularizer)\n",
    "        self.u_regularizer = regularizers.get(u_regularizer)\n",
    "        self.b_regularizer = regularizers.get(b_regularizer)\n",
    "        \n",
    "        self.W_constraint = constraints.get(W_constraint)\n",
    "        self.u_constraint = constraints.get(u_constraint)\n",
    "        self.b_constraint = constraints.get(b_constraint)\n",
    "\n",
    "        self.bias = bias\n",
    "        super(AttentionWithContext, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape) == 3\n",
    "\n",
    "        self.W = self.add_weight((input_shape[-1], input_shape[-1],),\n",
    "                                 initializer=self.init,\n",
    "                                 name='{}_W'.format(self.name),\n",
    "                                 regularizer=self.W_regularizer,\n",
    "                                 constraint=self.W_constraint)\n",
    "        if self.bias:\n",
    "            self.b = self.add_weight((input_shape[-1],),\n",
    "                                     initializer='zero',\n",
    "                                     name='{}_b'.format(self.name),\n",
    "                                     regularizer=self.b_regularizer,\n",
    "                                     constraint=self.b_constraint)\n",
    "        self.u = self.add_weight((input_shape[-1],),\n",
    "                                 initializer=self.init,\n",
    "                                 name='{}_u'.format(self.name),\n",
    "                                 regularizer=self.u_regularizer,\n",
    "                                 constraint=self.u_constraint)\n",
    "\n",
    "        super(AttentionWithContext, self).build(input_shape)\n",
    "\n",
    "    def compute_mask(self, input, input_mask=None):\n",
    "        # do not pass the mask to the next layers\n",
    "        return None\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        uit = dot_product(x, self.W)\n",
    "\n",
    "        if self.bias:\n",
    "            uit += self.b\n",
    "\n",
    "        uit = K.tanh(uit)\n",
    "        ait = dot_product(uit, self.u)\n",
    "\n",
    "        #a = K.exp(ait)\n",
    "        a = K.softmax(ait)\n",
    "        # apply mask after the exp. will be re-normalized next\n",
    "        #if mask is not None:\n",
    "        #    # Cast the mask to floatX to avoid float64 upcasting in theano\n",
    "        #    a *= K.cast(mask, K.floatx())\n",
    "\n",
    "        # in some cases especially in the early stages of training the sum may be almost zero\n",
    "        # and this results in NaN's. A workaround is to add a very small positive number Îµ to the sum.\n",
    "        # a /= K.cast(K.sum(a, axis=1, keepdims=True), K.floatx())\n",
    "        a = K.expand_dims(a)\n",
    "        weighted_input = x * a\n",
    "        return K.sum(weighted_input, axis=1)\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape[0], input_shape[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 400)               0         \n",
      "_________________________________________________________________\n",
      "embedding_1 (Embedding)      (None, 400, 100)          10835300  \n",
      "_________________________________________________________________\n",
      "spatial_dropout1d_1 (Spatial (None, 400, 100)          0         \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 400, 256)          234496    \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 400, 256)          1024      \n",
      "_________________________________________________________________\n",
      "attention_with_context_1 (At (None, 256)               66048     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 2)                 514       \n",
      "=================================================================\n",
      "Total params: 11,137,382\n",
      "Trainable params: 11,136,870\n",
      "Non-trainable params: 512\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "input_layer = Input(shape=(MAX_SEQUENCE_LENGTH,),dtype='int32')\n",
    "embedding_layer = Embedding(input_dim=len(word_index)+1,\n",
    "        output_dim=EMBEDDING_DIM,\n",
    "        input_length=MAX_SEQUENCE_LENGTH,\n",
    "        weights=[embedding_matrix],\n",
    "        embeddings_regularizer=regularizers.l2(0.00),\n",
    "        trainable=True)(input_layer)\n",
    "drop1 = SpatialDropout1D(0.3)(embedding_layer)\n",
    "lstm_1 = Bidirectional(LSTM(128, name='blstm_1',\n",
    "    activation='tanh',\n",
    "    recurrent_activation='hard_sigmoid',\n",
    "    recurrent_dropout=0.0,\n",
    "    dropout=0.5,\n",
    "    kernel_initializer='glorot_uniform',\n",
    "    return_sequences=True),\n",
    "    merge_mode='concat')(drop1)\n",
    "lstm_1 = BatchNormalization()(lstm_1)\n",
    "att_layer = AttentionWithContext()(lstm_1)\n",
    "drop3 = Dropout(0.5)(att_layer)\n",
    "predictions = Dense(2, activation='sigmoid')(drop3)\n",
    "model = Model(inputs=input_layer, outputs=predictions)\n",
    "\n",
    "model.summary()\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.load_weights('/Users/venkatakrishnamohansunkara/Desktop/DM/model_bi_lstm.weights.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 400, 256)\n"
     ]
    }
   ],
   "source": [
    "get_layer_output = K.function([model.layers[0].input, K.learning_phase()], [model.layers[4].output])\n",
    "out = get_layer_output([x_train[4567:4568],0])  # test mode\n",
    "print(out[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "eij = np.tanh(np.dot(out[0],model.layers[5].get_weights()[0]))\n",
    "t = np.dot(eij,model.layers[5].get_weights()[2])\n",
    "ai = np.exp(t)\n",
    "weights = ai/np.sum(ai)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 400)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gurgaon-->0.8232082473114133 \n",
      "agitating-->0.52955663704779 \n",
      "lecturers-->0.23384272935800254 \n",
      "called-->0.3280951932538301 \n",
      "off-->0.300654282909818 \n",
      "their-->0.4169056046521291 \n",
      "statewide-->0.6617409235332161 \n",
      "strike-->0.16963982488960028 \n",
      "on-->0.3416162871872075 \n",
      "saturday-->0.21803072741022334 \n",
      "after-->0.17520276742288843 \n",
      "the-->0.3250337249482982 \n",
      "director-->0.32167296012630686 \n",
      "general-->0.4257334876456298 \n",
      "of-->0.26901743694907054 \n",
      "higher-->0.23834654712118208 \n",
      "education-->0.31716808734927326 \n",
      "dghe-->0.4826814620173536 \n",
      "promised-->0.22978714696364477 \n",
      "to-->0.1839655487856362 \n",
      "look-->0.44891821744386107 \n",
      "into-->0.4781209645443596 \n",
      "their-->0.49056121497415006 \n",
      "demands-->0.47319699660874903 \n",
      "and-->0.13036752534389962 \n",
      "address-->0.36598536098608747 \n",
      "their-->0.27159014280186966 \n",
      "grievances-->0.5909315586904995 \n",
      "the-->0.2673931885510683 \n",
      "development-->0.23472846805816516 \n",
      "paved-->0.224234663619427 \n",
      "the-->0.16983452951535583 \n",
      "way-->0.22595591872232035 \n",
      "for-->0.2221556133008562 \n",
      "regular-->0.2998092895722948 \n",
      "classes-->0.5013035479350947 \n",
      "at-->3.971377736888826 \n",
      "government-->0.19596333004301414 \n",
      "colleges-->0.4921306754113175 \n",
      "in-->0.674235780024901 \n",
      "gurgaon-->1.2124719069106504 \n",
      "with-->0.20931947801727802 \n",
      "around-->0.5959936606814153 \n",
      "250-->0.33742395316949114 \n",
      "'extension'-->0.30984298064140603 \n",
      "guest-->0.3742441549547948 \n",
      "teachers-->0.25579214707249776 \n",
      "planning-->0.40711714973440394 \n",
      "to-->0.268204057647381 \n",
      "resume-->0.2718553878366947 \n",
      "work-->0.13953370398667175 \n",
      "on-->0.2092524664476514 \n",
      "monday-->0.2508558100089431 \n",
      "a-->0.3731108517968096 \n",
      "delegation-->0.6016808765707538 \n",
      "of-->0.25305318558821455 \n",
      "extension-->0.2785404103633482 \n",
      "lecturers'-->0.4353760596131906 \n",
      "association-->0.350231202901341 \n",
      "met-->0.2862855217244942 \n",
      "dghe-->0.5146049443283118 \n",
      "a-->0.47346879000542685 \n",
      "sreenivasan-->0.3631186700658873 \n",
      "at-->0.436661321145948 \n",
      "shiksa-->0.46451161324512213 \n",
      "sadan-->0.7027399260550737 \n",
      "on-->0.15855059245950542 \n",
      "saturday-->0.21016787286498584 \n",
      "to-->0.1864565820142161 \n",
      "press-->0.2920153201557696 \n",
      "for-->0.1517501277703559 \n",
      "better-->0.41056373447645456 \n",
      "remuneration-->0.6575375300599262 \n",
      "the-->0.20362216673674993 \n",
      "dghe-->0.34580749343149364 \n",
      "has-->0.09869352652458474 \n",
      "admitted-->0.42436582589289173 \n",
      "our-->0.6735191709594801 \n",
      "demands-->0.31527124519925565 \n",
      "are-->0.20907198631903157 \n",
      "legitimate-->0.48749177949503064 \n",
      "and-->0.1934765896294266 \n",
      "promised-->0.3391614882275462 \n",
      "to-->0.2143122947018128 \n",
      "fulfil-->0.3952401675633155 \n",
      "them-->0.1594537025084719 \n",
      "so-->0.1629680446058046 \n",
      "we-->0.27970727387582883 \n",
      "are-->0.18730273950495757 \n",
      "calling-->0.5024857455282472 \n",
      "off-->0.2327961556147784 \n",
      "the-->0.2601089363452047 \n",
      "stir-->0.7711942453170195 \n",
      "and-->0.14963899957365356 \n",
      "classes-->0.223229708353756 \n",
      "will-->0.13944926649855915 \n",
      "be-->0.22029527826816775 \n",
      "resumed-->0.2447592851240188 \n",
      "from-->0.1525995321571827 \n",
      "monday-->0.12694205906882416 \n",
      "we-->0.19943649022025056 \n",
      "are-->0.12481541489250958 \n",
      "equally-->0.2608457907626871 \n",
      "concerned-->0.399176606151741 \n",
      "about-->0.24149197997758165 \n",
      "students-->0.14289660612121224 \n",
      "whose-->0.14734239812241867 \n",
      "exams-->0.11291283954051323 \n",
      "are-->0.11367629667802248 \n",
      "round-->0.3312020271550864 \n",
      "the-->0.19978684576926753 \n",
      "corner-->0.27735633921111 \n",
      "they-->0.21932235540589318 \n",
      "education-->0.25239749447791837 \n",
      "department-->0.30867387977195904 \n",
      "officials-->0.1403018268320011 \n",
      "asked-->0.3624216333264485 \n",
      "for-->0.15160832845140249 \n",
      "two-->0.17039294107235037 \n",
      "days-->0.14322452443593647 \n",
      "and-->0.08797588634479325 \n",
      "we-->0.28956812457181513 \n",
      "have-->0.1788152258086484 \n",
      "given-->0.20102306734770536 \n",
      "them-->0.24111348466249183 \n",
      "seven-->0.2580595901235938 \n",
      "now-->0.15978710507624783 \n",
      "we-->0.339249090757221 \n",
      "don't-->0.3886323975166306 \n",
      "want-->0.3746358197531663 \n",
      "to-->0.17814103557611816 \n",
      "be-->0.12749706002068706 \n",
      "disappointed-->0.4672310024034232 \n",
      "if-->0.29728180379606783 \n",
      "they-->0.24425997253274545 \n",
      "don't-->0.3363292125868611 \n",
      "do-->0.2351912553422153 \n",
      "anything-->0.6153021240606904 \n",
      "in-->0.2775717439362779 \n",
      "seven-->0.24359153030673042 \n",
      "days-->0.1387661450280575 \n",
      "we-->0.3456618287600577 \n",
      "will-->0.2197986759711057 \n",
      "resume-->0.36786284908885136 \n",
      "our-->2.0166060130577534 \n",
      "strike-->0.09486617273068987 \n",
      "sumer-->0.40257222281070426 \n",
      "singh-->1.0760905570350587 \n",
      "spokesperson-->0.5468321614898741 \n",
      "for-->0.16417789083789103 \n",
      "the-->0.2599225990707055 \n",
      "lecturers'-->0.4236568929627538 \n",
      "association-->0.21574429410975426 \n",
      "said-->0.22813708710600622 \n",
      "sources-->0.2178893737436738 \n",
      "said-->0.28219154046382755 \n",
      "a-->0.40246592106996104 \n",
      "delegation-->0.6491287058452144 \n",
      "of-->0.30844217690173537 \n",
      "teachers-->0.207351131393807 \n",
      "was-->0.16901230992516503 \n",
      "supposed-->0.2523171133361757 \n",
      "to-->0.33315722248516977 \n",
      "meet-->0.6004710667184554 \n",
      "chief-->0.7981192175066099 \n",
      "minister-->1.4232151443138719 \n",
      "mahohar-->0.9782698180060834 \n",
      "lal-->1.390419201925397 \n",
      "khattar-->0.42889609176199883 \n",
      "on-->0.11497816558403429 \n",
      "friday-->0.17825641407398507 \n",
      "however-->0.1237955530086765 \n",
      "he-->0.16739226339268498 \n",
      "was-->0.14638731954619288 \n",
      "not-->0.1372591850667959 \n",
      "available-->0.17950567780644633 \n",
      "instead-->0.276854098046897 \n",
      "the-->0.21161071344977245 \n",
      "lectures-->0.45721131755271927 \n",
      "met-->0.24330700398422778 \n",
      "bopeshwar-->0.5531975330086425 \n",
      "dayal-->0.23591706849401817 \n",
      "the-->0.23448494175681844 \n",
      "officer-->0.2950595080619678 \n",
      "on-->0.16247591702267528 \n",
      "special-->0.37006018828833476 \n",
      "duty-->0.24637713067932054 \n",
      "to-->0.1697070365480613 \n",
      "cm-->0.690535525791347 \n",
      "he-->0.1713085475785192 \n",
      "persuaded-->0.37880185118410736 \n",
      "them-->0.1620924740564078 \n",
      "to-->0.19451625121291727 \n",
      "call-->0.33619668101891875 \n",
      "off-->0.19455799701972865 \n",
      "their-->0.7027098763501272 \n",
      "strike-->0.18885602912632748 \n",
      "and-->0.15418449038406834 \n",
      "promised-->0.14182332051859703 \n",
      "to-->0.08359109415323474 \n",
      "meet-->0.11188952157681342 \n",
      "their-->0.09973232408810873 \n",
      "demands-->0.12389893527142704 \n",
      "said-->0.07620913947903318 \n",
      "a-->0.09895215953292791 \n",
      "source-->0.2475427208992187 \n",
      "agitating-->0.3363832729519345 \n",
      "under-->0.1036904905049596 \n",
      "the-->0.12270408660697285 \n",
      "banner-->0.14858024769637268 \n",
      "of-->0.25103488951572217 \n",
      "all-->0.2545459028624464 \n",
      "haryana-->1.119540975196287 \n",
      "extension-->0.18519760487833992 \n",
      "lecturers'-->0.26196214093943126 \n",
      "association-->0.17751863197190687 \n",
      "that-->0.2632526411616709 \n",
      "launched-->0.36462442949414253 \n",
      "a-->0.28483394999057055 \n",
      "state-->0.4522327799350023 \n",
      "level-->1.3214632053859532 \n",
      "strike-->0.23416007024934515 \n",
      "last-->1.1519267718540505 \n",
      "month-->1.014994631987065 \n",
      "the-->1.1873913172166795 \n",
      "lecturers-->2.8895068680867553 \n",
      "alleged-->2.4366972502321005 \n",
      "the-->10.99171000532806 \n",
      "government-->0.7274637027876452 \n",
      "had-->0.9420391143066809 \n",
      "failed-->1.898769842227921 \n",
      "to-->2.1210672275628895 \n",
      "implement-->4.804937925655395 \n",
      "the-->2.690811234060675 \n",
      "punjab-->11.005067499354482 \n",
      "and-->2.201291499659419 \n",
      "haryana-->11.076712980866432 \n",
      "high-->1.1967134196311235 \n",
      "court's-->3.858230193145573 \n",
      "directive-->2.5698551326058805 \n",
      "dated-->2.92042299406603 \n",
      "may-->1.6839010640978813 \n",
      "5-->1.8183636711910367 \n",
      "2016-->2.4077281705103815 \n",
      "to-->2.160451258532703 \n",
      "pay-->3.0685425736010075 \n",
      "extension-->4.724357859231532 \n",
      "lecturers-->11.973729124292731 \n",
      "as-->5.310602718964219 \n",
      "per-->11.652855901047587 \n",
      "ucg-->121.49261310696602 \n",
      "norms-->51.99183709919453 \n"
     ]
    }
   ],
   "source": [
    "id2word = {v: k for k, v in word_index.items()}\n",
    "with open(\"visualization_bilstm.html\", \"w\") as html_file:\n",
    "    html_file.write('<!DOCTYPE html>\\n')\n",
    "    html_file.write('<html>\\n')\n",
    "    html_file.write('<body>\\n')\n",
    "    word_no = 0\n",
    "    for i in x_train[4567]:\n",
    "        if i!=0:\n",
    "        # Iterate through the words in the sentence\n",
    "            word = id2word[i]\n",
    "            alpha = weights[0][word_no]*10000\n",
    "            html_file.write('<font style=\"background-color: rgba(255, 0, 0, %f)\">%s</font>\\n' %(alpha,word))\n",
    "            print(word+'-->'+str(alpha)+' ')\n",
    "            word_no+=1\n",
    "    html_file.write('</body>\\n')\n",
    "    html_file.write('</html>\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_test_df['Bi-LSTM embeddings'] = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/envs/textClassifier/lib/python3.6/site-packages/ipykernel_launcher.py:4: FutureWarning: set_value is deprecated and will be removed in a future release. Please use .at[] or .iat[] accessors instead\n",
      "  after removing the cwd from sys.path.\n"
     ]
    }
   ],
   "source": [
    "doc_output = K.function([model.layers[0].input, K.learning_phase()], [model.layers[5].output])\n",
    "out = doc_output([X_data_test,0])\n",
    "for e,i in enumerate(out[0]):\n",
    "    data_test_df.set_value(e,'Bi-LSTM embeddings',i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_test_df['bi-lstmcosine'] = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from numpy import dot\n",
    "from numpy.linalg import norm\n",
    "\n",
    "def cos_sim(a,b):\n",
    "    return dot(a, b)/(norm(a)*norm(b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/envs/textClassifier/lib/python3.6/site-packages/ipykernel_launcher.py:6: FutureWarning: set_value is deprecated and will be removed in a future release. Please use .at[] or .iat[] accessors instead\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "for i,row1 in data_test_df.iterrows():\n",
    "    l= []\n",
    "    for j,row2 in data_test_df.iterrows():\n",
    "        cs = cos_sim(row1['Bi-LSTM embeddings'],row2['Bi-LSTM embeddings'])\n",
    "        l+=[cs]\n",
    "    data_test_df.set_value(i,'bi-lstmcosine',np.asarray(l))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "[1.         0.96025157 0.9528559 ]\n",
      "[0 6 2]\n",
      "1\n",
      "[1.        0.9716942 0.9631069]\n",
      "[1 7 5]\n",
      "2\n",
      "[0.9999999 0.9528559 0.9153167]\n",
      "[2 0 1]\n",
      "3\n",
      "[1.         0.98588735 0.96697867]\n",
      "[3 6 4]\n",
      "4\n",
      "[1.         0.98129034 0.97444236]\n",
      "[4 6 7]\n",
      "5\n",
      "[1.        0.9778861 0.9631069]\n",
      "[5 7 1]\n",
      "6\n",
      "[1.         0.98588735 0.98129034]\n",
      "[6 3 4]\n",
      "7\n",
      "[0.99999994 0.9778861  0.97444236]\n",
      "[7 5 4]\n",
      "8\n",
      "[1.         0.79635936 0.7540187 ]\n",
      "[8 1 5]\n"
     ]
    }
   ],
   "source": [
    "for i,row in data_test_df.iterrows():\n",
    "    print(i);\n",
    "    r = np.sort(row['bi-lstmcosine'])[::-1][0:3]\n",
    "    r2 = np.argsort(row['bi-lstmcosine'])[::-1][0:3]\n",
    "    print(r)\n",
    "    print(r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (textClassifier)",
   "language": "python",
   "name": "textclassifier"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
