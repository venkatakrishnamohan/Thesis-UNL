{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from mediawiki import MediaWiki\n",
    "from bs4 import BeautifulSoup\n",
    "import string\n",
    "from nltk import tokenize\n",
    "from nltk.tag import StanfordNERTagger\n",
    "import string\n",
    "from nltk.tokenize import word_tokenize\n",
    "import unicodedata\n",
    "import timeit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Using the Stanfors' default tagger\n",
    "st = StanfordNERTagger('/Users/venkatakrishnamohansunkara/Desktop/DM/stanford-ner-2018-02-27/classifiers/english.all.3class.distsim.crf.ser.gz',\n",
    "  '/Users/venkatakrishnamohansunkara/Desktop/DM/stanford-ner-2018-02-27/stanford-ner.jar',\n",
    "   encoding='utf-8') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Lists containing common prefixes and suffixes.\n",
    "prefix = ['mr','by','mrs','miss','dr', 'president','prime minister']\n",
    "# https://www.irfca.org/docs/place-names.html\n",
    "#suffixes = ['nagar','colony','street','road','hill','river','temple']\n",
    "#subs = ['pur','puram','ur','ghar','pura','ganj','abad','halli','keri']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#news_data = pd.read_csv('/Users/venkatakrishnamohansunkara/Desktop/DM/GDELT_2017/large_dataset/unrest_2017_ie_toi.csv', sep=',')\n",
    "df = pd.read_csv('../document similarity/document_similarity_hatt.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_data = df.iloc[5].content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Function for removing unnecessary characters.\n",
    "def clean_str(string):\n",
    "    \"\"\"\n",
    "    Tokenization/string cleaning for dataset\n",
    "    Every dataset is lower cased except\n",
    "    \"\"\"\n",
    "    string = re.sub(r\"\\\\\", \" \", string)\n",
    "    string = re.sub(r\"\\'\", \" \", string)\n",
    "    string = re.sub(r\"\\\"\", \" \", string)\n",
    "    string = re.sub(r\"\\n\", \" \", string)\n",
    "    string = re.sub(r\"=\", \" \", string)\n",
    "    string = re.sub(r\"-\", \" \", string)\n",
    "    string = re.sub(r\"/\", \" \", string)\n",
    "    return string.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import en_coref_lg\n",
    "nlp = en_coref_lg.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "preps_score={'\"': 0.99,\n",
    " 'down': 0.99,\n",
    " 'across': 0.25,\n",
    " 'near': 0.67,\n",
    " 'inside': 0.23,\n",
    " 'amid': 0.23,\n",
    " 'in': 0.22,\n",
    " 'from': 0.205,\n",
    " 'off': 0.2,\n",
    " 'at': 0.19,\n",
    " 'like': 0.18,\n",
    " 'outside': 0.12,\n",
    " 'because': 0.1,\n",
    " 'of': 0.09,\n",
    " 'about': 0.08,\n",
    " 'between': 0.075,\n",
    " 'by': 0.07,\n",
    " 'among': 0.07,\n",
    " 'with': 0.07,\n",
    " 'against': 0.07,\n",
    " 'into': 0.06,\n",
    " 'that': 0.05,\n",
    " 'through': 0.045,\n",
    " 'during':0.045,\n",
    " 'after': 0.045,\n",
    " 'on': 0.03,\n",
    " 'while':0.03,\n",
    " 'for': 0.02,\n",
    " 'as': 0.015}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vocab = [\"strike\",\n",
    "\"unrest\",\n",
    "\"masses\",\n",
    "\"protest\",\n",
    "\"demonstration\",\n",
    "\"worker\",\n",
    "\"union\",\n",
    "\"company\",\n",
    "\"caste\",\n",
    "\"religious\",\n",
    "\"ethnic\",\n",
    "\"reformed\",\n",
    "\"rebellion\",\n",
    "\"defense\",\n",
    "\"violence\",\n",
    "\"war\",\n",
    "\"armed\",\n",
    "\"fight\",\n",
    "\"Right\",\n",
    "\"free\",\n",
    "\"freedom\",\n",
    "\"liberty\",\n",
    "\"justice\",\n",
    "\"Fair\",\n",
    "\"unfair\",\n",
    "\"unequal\",\n",
    "\"terror\",\n",
    "\"extreme\",\n",
    "\"Bomb\",\n",
    "\"IED\",\n",
    "\"weapon\",\n",
    "\"gun\",\n",
    "\"wmd\",\n",
    "\"threat\",\n",
    "\"suicide\",\n",
    "\"murder\",\n",
    "\"Kill\",\n",
    "\"death\",\n",
    "\"explosive\",\n",
    "\"military\",\n",
    "\"police\",\n",
    "\"elite\",\n",
    "\"government\",\n",
    "\"oppresive\",\n",
    "\"power\",\n",
    "\"regime\",\n",
    "\"fraud\",\n",
    "\"corruption\",\n",
    "\"coup\",\n",
    "\"safety\",\n",
    "\"secure\",\n",
    "\"insecure\",\n",
    "\"protect\",\n",
    "\"enemy\",\n",
    "\"resist\",\n",
    "\"hostage\",\n",
    "\"truce\",\n",
    "\"fire\",\n",
    "\"greed\",\n",
    "\"panic\",\n",
    "\"inflation\",\n",
    "\"Price\",\n",
    "\"Food\",\n",
    "\"Water\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_whos(test_data):\n",
    "    # INCLUDE NOUN PHRASES ALSO AS CANDIDATES OR CERTAIN WORDS LIKE GOVERNMENT.\n",
    "    start = timeit.default_timer()\n",
    "    # Convert the entire document data to sentences.\n",
    "    text_sent = nltk.sent_tokenize(test_data)\n",
    "    stanford = []\n",
    "    noun_phrases = []\n",
    "    prp_phrases = []\n",
    "    #new_model = []\n",
    "    whos = []\n",
    "    # other are prefixes\n",
    "    other = []\n",
    "    #suffix_locations = []\n",
    "    #sub_locations = []\n",
    "    all_words = []\n",
    "    # Iterate over all the sentences.\n",
    "    sent_no=0;\n",
    "    total_words = 0;\n",
    "    for sent in text_sent:\n",
    "        # Convert the sentence to words\n",
    "        text_tags = nltk.word_tokenize(sent)\n",
    "        curr_words = len(text_tags)\n",
    "        all_words+=[text_tags]\n",
    "        # Get the NER for the words.\n",
    "        original_tags = st.tag(text_tags)\n",
    "        l = len(original_tags)\n",
    "        i=0;\n",
    "        # Iterate over the tagged words.\n",
    "        while i<l:\n",
    "            #print(i)\n",
    "            e,t = original_tags[i];\n",
    "            ci = i;\n",
    "            # If it's a location, then check the next 3 words.\n",
    "            if t == 'PERSON' or t == 'ORGANIZATION':\n",
    "                j = 1;\n",
    "                s = e; \n",
    "                # Verify the tags for the next 3 words.\n",
    "                while i+j<len(original_tags):\n",
    "                    # If the next words are also locations, then concatenate them to make a large string.\n",
    "                    if original_tags[i+j][1] == 'PERSON' or original_tags[i+j][1] == 'ORGANIZATION':\n",
    "                        s = s+\" \"+original_tags[i+j][0];\n",
    "                        j+=1;\n",
    "                    else:\n",
    "                        break;\n",
    "                i = i+j;\n",
    "                # Save the candidates to a whos list\n",
    "                whos+=[(sent_no,total_words+ci,s,ci)];\n",
    "            else:\n",
    "                i=i+1;\n",
    "        # Get the POS tag for the words.\n",
    "        text_tagged = nltk.pos_tag(text_tags)\n",
    "        l = len(text_tagged)\n",
    "        i=0;\n",
    "        # iterate over the tagged words.\n",
    "        while i<l:\n",
    "            #print(i)\n",
    "            e,t = text_tagged[i];\n",
    "            # If the current word is a Noun phrase.\n",
    "            if t == 'NNP':\n",
    "                j = 1;\n",
    "                s = e;\n",
    "                # verify the tags for the next 3 words.\n",
    "                while i+j<len(text_tagged):\n",
    "                    # If the next words are also nouns, then concatenate them to make a large string.\n",
    "                    if text_tagged[i+j][1] in ['NNP','NN','NNS','NNPS']:\n",
    "                        s = s+\" \"+text_tagged[i+j][0];\n",
    "                        j+=1;\n",
    "                    else:\n",
    "                        break;\n",
    "                i = i+j;\n",
    "                # Save the noun phrases to a noun_phrases list\n",
    "                noun_phrases+=[s];\n",
    "            elif t == 'PRP' or t == 'PRP$' or t == 'WP' or t == 'WP$':\n",
    "                j = 1;\n",
    "                s = e;\n",
    "                # verify the tags for the next 3 words.\n",
    "                while i+j<len(text_tagged):\n",
    "                    # If the next words are also nouns, then concatenate them to make a large string.\n",
    "                    if text_tagged[i+j][1] in ['PRP','PRP$','WP','WP$']:\n",
    "                        s = s+\" \"+text_tagged[i+j][0];\n",
    "                        j+=1;\n",
    "                    else:\n",
    "                        break;\n",
    "                i = i+j;\n",
    "                # Save the noun phrases to a noun_phrases list\n",
    "                prp_phrases+=[s];\n",
    "            else:\n",
    "                i=i+1;\n",
    "        # Convert the words to lower case so as to compare with the prefixes.\n",
    "        text_tags = [t.lower() for t in text_tags]\n",
    "        # Go through all the prefixes.\n",
    "        for k in prefix:\n",
    "            # If the prefix is not present in the sentence then an error occurs. So, we use a try, catch block\n",
    "            try:\n",
    "                # Get the position the prefix word in a sentence.\n",
    "                index = text_tags.index(k)\n",
    "                # If the word after the prefix word is a noun, then consider it.\n",
    "                if text_tagged[index+1][1] == 'NNP':\n",
    "                    j = 1;\n",
    "                    s = text_tagged[index+1][0]; \n",
    "                    # Verify if it is a phrase by considering the next 3 words.\n",
    "                    while index+j<len(text_tagged):\n",
    "                        if text_tagged[index+j+1][1] in ['NNP','NN','NNS','NNPS']:\n",
    "                            s = s+\" \"+text_tagged[index+j+1][0];\n",
    "                            j+=1;\n",
    "                        else:\n",
    "                            break;\n",
    "                    # Save the words after the prefix words in to other list.\n",
    "                    other = other + [(sent_no,index+1,s)]\n",
    "            except:\n",
    "                continue\n",
    "        sent_no+=1;\n",
    "        total_words+=curr_words\n",
    "    stop = timeit.default_timer()\n",
    "    print('Time: ', stop - start) \n",
    "    all_words = [x for sublist in all_words for x in sublist]\n",
    "    #only by stanford ner\n",
    "    return all_words, whos, text_sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_unrest_whos(all_words, whos):\n",
    "    pos_threshold = len(all_words)\n",
    "    all_whos = {k[2]:[] for k in whos}\n",
    "    #nearest_whos = {k[2]:[] for k in whos}\n",
    "    pos_occurences = {k[2]:[] for k in whos}\n",
    "    pos_scores = {k:-1 for k in pos_occurences.keys()}\n",
    "    unrest_scores = {k:-1 for k in all_whos.keys()}\n",
    "    for location in list(all_whos.keys()):\n",
    "        sub_locations = location.split(' ')\n",
    "        if len(sub_locations) > 1:\n",
    "            last_subloc = sub_locations[-1]\n",
    "            first_subloc = sub_locations[0]\n",
    "            first_indices = [i for i, w in enumerate(all_words) if w == first_subloc]\n",
    "            last_indices = [i for i, w in enumerate(all_words) if w == last_subloc]\n",
    "            diff = [a_i - b_i for a_i, b_i in zip(last_indices, first_indices)]\n",
    "            first_indices = first_indices[diff == len(sub_locations)]\n",
    "            if not isinstance(first_indices,list):\n",
    "                pos_occurences[location].append(first_indices)\n",
    "            else:\n",
    "                pos_occurences[location] = first_indices\n",
    "            indices = pos_occurences[location]\n",
    "        else:\n",
    "            first_indices = [i for i, w in enumerate(all_words) if w == sub_locations[0]]\n",
    "            if not isinstance(first_indices,list):\n",
    "                pos_occurences[location].append(first_indices)\n",
    "            else:\n",
    "                pos_occurences[location] = first_indices\n",
    "            indices = pos_occurences[location]\n",
    "        print(location)\n",
    "        print(indices)\n",
    "        if len(indices)>0:\n",
    "            no_of_indices = len(indices)\n",
    "            i=0\n",
    "            b=[]\n",
    "            #nearest_whos = []\n",
    "            while i< no_of_indices:\n",
    "                j=0\n",
    "                a = []\n",
    "                l = []\n",
    "                while j<pos_threshold and indices[i]+j<len(all_words):\n",
    "                    if all_words[indices[i]+j].lower() in vocab:\n",
    "                        s = [j,indices[i],'r',all_words[indices[i]+j].lower()]\n",
    "                        a.append(s)\n",
    "                    if all_words[indices[i]+j] in all_whos:\n",
    "                        lo = [j,indices[i],'r',all_words[indices[i]+j].lower()]\n",
    "                        l.append(lo)\n",
    "                    j=j+1\n",
    "                j=0\n",
    "                while j<pos_threshold and indices[i]-j>=0:\n",
    "                    if all_words[indices[i]-j].lower() in vocab:\n",
    "                        s = [j,indices[i],'l',all_words[indices[i]-j].lower()]\n",
    "                        a.append(s)\n",
    "                    if all_words[indices[i]-j] in all_whos:\n",
    "                        lo = [j,indices[i],'l',all_words[indices[i]-j].lower()]\n",
    "                        l.append(lo)\n",
    "                    j=j+1\n",
    "                i=i+1\n",
    "                b = b+a\n",
    "                #nearest_whos = nearest_whos+l\n",
    "            all_whos[location] = b\n",
    "    return all_whos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def findWholeWord(w):\n",
    "    return re.compile(r'\\b({0})\\b'.format(w), flags=re.IGNORECASE).search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_modified_whos(all_whos):\n",
    "    i=0;\n",
    "    whos = {}\n",
    "    while i<len(all_whos):\n",
    "        if all_whos[i][2] not in whos:\n",
    "            whos[all_whos[i][2]] = [(all_whos[i][0],all_whos[i][1],all_whos[i][3])]\n",
    "        j=i+1;\n",
    "        while j < len(all_whos):\n",
    "            if all_whos[j][2].lower() == all_whos[i][2].lower():\n",
    "                whos[all_whos[i][2]]+=[(all_whos[j][0],all_whos[j][1],all_whos[j][3])]\n",
    "                all_whos.pop(j);\n",
    "                #do not increment j\n",
    "                continue;\n",
    "            if findWholeWord(all_whos[j][2].lower())(all_whos[i][2].lower()) or findWholeWord(all_whos[i][2].lower())(all_whos[j][2].lower()):\n",
    "                whos[all_whos[i][2]]+=[(all_whos[j][0],all_whos[j][1],all_whos[j][3])]\n",
    "                all_whos.pop(j);\n",
    "                continue;\n",
    "            j+=1;\n",
    "        i+=1\n",
    "    return whos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#avg is considered\n",
    "def get_pos_sent_scores(modified_who_candidates, all_words, text_sent):\n",
    "    pos_scores = {k:0 for k in modified_who_candidates.keys()}\n",
    "    sent_scores = {k:0 for k in modified_who_candidates.keys()}\n",
    "    l = len(all_words)\n",
    "    n = len(text_sent)\n",
    "    for s in modified_who_candidates.keys():\n",
    "        pos = modified_who_candidates[s];\n",
    "        pos_score = 0;\n",
    "        sent_score = 0;\n",
    "        for p in pos:\n",
    "            pos_score+=(1 - p[1]/l)\n",
    "            sent_score+=(1 - p[0]/n)\n",
    "        pos_score = pos_score/len(pos)\n",
    "        sent_score = sent_score/len(pos)\n",
    "        pos_scores[s] = pos_score\n",
    "        sent_scores[s] = sent_score\n",
    "\n",
    "    return pos_scores,sent_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_modified_unrest_whos(all_whos):\n",
    "    i=0;\n",
    "    whos = {}\n",
    "    who_keys = sorted(all_whos, key=len, reverse=True)\n",
    "    while i<len(who_keys):\n",
    "        if who_keys[i] not in whos:\n",
    "            print(who_keys[i])\n",
    "            print('-----------')\n",
    "            whos[who_keys[i]] = all_whos[who_keys[i]]\n",
    "        j=i+1;\n",
    "        #t = [];\n",
    "        while j < len(who_keys):\n",
    "            #print(who_keys[i])\n",
    "            if who_keys[i].lower() == who_keys[j].lower():\n",
    "                whos[who_keys[i]].extend(all_whos[who_keys[j]])\n",
    "                who_keys.pop(j);\n",
    "                #do not increment j\n",
    "                continue;\n",
    "            if findWholeWord(who_keys[i].lower())(who_keys[j].lower()) or findWholeWord(who_keys[j].lower())(who_keys[i].lower()):\n",
    "                whos[who_keys[i]].extend(all_whos[who_keys[j]])\n",
    "                print(who_keys[i])\n",
    "                print(who_keys[j])\n",
    "                who_keys.pop(j);\n",
    "                continue;\n",
    "            j+=1;\n",
    "        #if(len(t) !=0):\n",
    "        #    print('adding....')\n",
    "        #print(t)\n",
    "        #whos[who_keys[i]].extend(t)\n",
    "        i+=1\n",
    "    return whos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_title_score(whos, title, ):\n",
    "    title_score = {k:0 for k in whos.keys()}\n",
    "    title_words = nltk.word_tokenize(title)\n",
    "    for k in title_score.keys():\n",
    "        for w in title_words:\n",
    "            if k.lower() == w.lower():\n",
    "                title_score[k] = 1;\n",
    "            if findWholeWord(k.lower())(w.lower()) or findWholeWord(w.lower())(k.lower()):\n",
    "                print(k)\n",
    "                print(w)\n",
    "                title_score[k] = 1;\n",
    "    return title_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_unrest_scores(modified_unrest_whos):\n",
    "    thresh = 10;\n",
    "    unrest_scores = {k:-1 for k in modified_unrest_whos.keys()}\n",
    "    for l in modified_unrest_whos.keys():\n",
    "        score =0;\n",
    "        if len(modified_unrest_whos[l]) >0:\n",
    "            locs_unrest = modified_unrest_whos[l]\n",
    "            for l2 in locs_unrest:\n",
    "                if l2[0]<thresh:\n",
    "                    score = score+(1/l2[0]);  \n",
    "                else:\n",
    "                    score = score+np.exp(-l2[0]); \n",
    "        unrest_scores[l] = score/len(modified_unrest_whos);\n",
    "    return unrest_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_frequency(modified_who_candidates):\n",
    "    frequecies = {k:-1 for k in modified_who_candidates.keys()}\n",
    "    for l in modified_who_candidates.keys:\n",
    "        frequecies[l] = len(modified_who_candidates[l])\n",
    "    return frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_sent_nos(modified_who_candidates):\n",
    "    sent_nos = {k:[] for k in modified_who_candidates.keys()}\n",
    "    for k in modified_who_candidates.keys():\n",
    "        a = modified_who_candidates[k]\n",
    "        c = []\n",
    "        for b in a:\n",
    "            c.extend([b[0]])\n",
    "        sent_nos[k] = set(c)\n",
    "    return sent_nos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_preps_scores(modified_who_candidates,all_words,preps_score):\n",
    "    preps_scores = {k:[] for k in modified_who_candidates.keys()}\n",
    "    for name in modified_who_candidates.keys():\n",
    "        indices = modified_who_candidates[name]\n",
    "        s = []\n",
    "        for tup in indices:\n",
    "            i=tup[1]\n",
    "            try:\n",
    "                if i-1>0 and all_words[i-1] in preps_score.keys():\n",
    "                    s.append(preps_score[all_words[i-1]])\n",
    "                elif i-2>0 and all_words[i-1] in prefix and all_words[i-1] in preps_score.keys():\n",
    "                    s.append(preps_score[all_words[i-2]])\n",
    "                else:\n",
    "                    s.append(0);   \n",
    "            except:\n",
    "                    continue\n",
    "        # Taking an average\n",
    "        preps_scores[name] = np.average(s)\n",
    "    # preps_scores: Scores to each location mention in an article based on the prepositions preceding them.\n",
    "    return preps_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_data = unicodedata.normalize(\"NFKD\", test_data)\n",
    "test_data = clean_str(test_data)\n",
    "doc = nlp(test_data)\n",
    "test_data = doc._.coref_resolved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time:  30.990341798999907\n"
     ]
    }
   ],
   "source": [
    " all_words, whos, text_sent = get_whos(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anna Hazare\n",
      "[0]\n",
      "Jan Lokpal Bill Veteran Gandhian\n",
      "[7]\n",
      "Jan Lokpal Bill\n",
      "[7]\n",
      "Mahatma Gandhi\n",
      "[62]\n",
      "Swami Agnivesh\n",
      "[76]\n",
      "Kiran Bedi\n",
      "[85]\n",
      "Sandeep Pandey\n",
      "[92]\n",
      "Hazare\n",
      "[1, 15, 131, 148, 184, 203, 234, 237, 240, 296, 300, 400]\n",
      "Prime Minister s Office\n",
      "[44]\n",
      "Bill\n",
      "[9, 34, 127, 285]\n",
      "Santosh Hegde\n",
      "[314]\n",
      "Prashant Bhushan\n",
      "[320]\n",
      "‘ India Against Corruption\n",
      "[341]\n"
     ]
    }
   ],
   "source": [
    "unrest_whos = get_unrest_whos(all_words,whos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "modified_who_candidates = get_modified_whos(whos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jan Lokpal Bill Veteran Gandhian\n",
      "-----------\n",
      "Jan Lokpal Bill Veteran Gandhian\n",
      "Jan Lokpal Bill\n",
      "Jan Lokpal Bill Veteran Gandhian\n",
      "Bill\n",
      "‘ India Against Corruption\n",
      "-----------\n",
      "Prime Minister s Office\n",
      "-----------\n",
      "Prashant Bhushan\n",
      "-----------\n",
      "Mahatma Gandhi\n",
      "-----------\n",
      "Swami Agnivesh\n",
      "-----------\n",
      "Sandeep Pandey\n",
      "-----------\n",
      "Santosh Hegde\n",
      "-----------\n",
      "Anna Hazare\n",
      "-----------\n",
      "Anna Hazare\n",
      "Hazare\n",
      "Kiran Bedi\n",
      "-----------\n"
     ]
    }
   ],
   "source": [
    "modifies_unrest_whos = get_modified_unrest_whos(unrest_whos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pos_scores,sent_scores=get_pos_sent_scores(modified_who_candidates,all_words,text_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anna Hazare\n",
      "Anna\n",
      "Anna Hazare\n",
      "Hazare\n",
      "Jan Lokpal Bill Veteran Gandhian\n",
      "Jan\n",
      "Jan Lokpal Bill Veteran Gandhian\n",
      "Lokpal\n",
      "Jan Lokpal Bill Veteran Gandhian\n",
      "Bill\n"
     ]
    }
   ],
   "source": [
    "title_scores = get_title_score(modified_who_candidates,\"Anna Hazare on fast unto death demanding Jan Lokpal Bill\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "unrest_scores = get_unrest_scores(modifies_unrest_whos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sent_nos = get_sent_nos(modified_who_candidates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "preps_scores = get_preps_scores(modified_who_candidates,all_words,preps_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_final_scores(pos_scores,sent_scores,unrest_scores,preps_scores,title_scores):\n",
    "    final_scores = {k:-1 for k in pos_scores.keys()}\n",
    "    for i in final_scores.keys():\n",
    "            score = pos_scores[i]+sent_scores[i]+unrest_scores[i]+preps_scores[i]+title_scores[i]\n",
    "            final_scores[i] = score/5;\n",
    "    # final_scores: A combination of all the above scores for a location.\n",
    "    return final_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "final_scores = get_final_scores(pos_scores,sent_scores,unrest_scores,preps_scores,title_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Generate dependency trees for each of the sentence in which a who candidate appears.\n",
    "import os\n",
    "from nltk.parse.stanford import StanfordDependencyParser\n",
    "from graphviz import Source\n",
    "\n",
    "# make sure nltk can find stanford-parser\n",
    "# please check your stanford-parser version from brew output (in my case 3.6.0) \n",
    "os.environ['CLASSPATH'] = r'/usr/local/Cellar/stanford-parser/3.9.1/libexec'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:6: DeprecationWarning: The StanfordDependencyParser will be deprecated\n",
      "Please use \u001b[91mnltk.parse.corenlp.CoreNLPDependencyParser\u001b[0m instead.\n",
      "  \n",
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:6: DeprecationWarning: The StanfordDependencyParser will be deprecated\n",
      "Please use \u001b[91mnltk.parse.corenlp.CoreNLPDependencyParser\u001b[0m instead.\n",
      "  \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-115-c796d15fb9bd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0msentence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtext_sent\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msent_no\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0msdp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mStanfordDependencyParser\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msdp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraw_parse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mdep_tree_dot_repr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mparse\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mparse\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_dot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/nltk/parse/stanford.py\u001b[0m in \u001b[0;36mraw_parse\u001b[0;34m(self, sentence, verbose)\u001b[0m\n\u001b[1;32m    156\u001b[0m         \u001b[0;34m:\u001b[0m\u001b[0mrtype\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTree\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m         \"\"\"\n\u001b[0;32m--> 158\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraw_parse_sents\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    159\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mraw_parse_sents\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/nltk/parse/stanford.py\u001b[0m in \u001b[0;36mraw_parse_sents\u001b[0;34m(self, sentences, verbose)\u001b[0m\n\u001b[1;32m    178\u001b[0m         ]\n\u001b[1;32m    179\u001b[0m         return self._parse_trees_output(\n\u001b[0;32m--> 180\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_execute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcmd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'\\n'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    181\u001b[0m         )\n\u001b[1;32m    182\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/nltk/parse/stanford.py\u001b[0m in \u001b[0;36m_execute\u001b[0;34m(self, cmd, input_, verbose)\u001b[0m\n\u001b[1;32m    264\u001b[0m                 \u001b[0mcmd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m                 stdout, stderr = java(\n\u001b[0;32m--> 266\u001b[0;31m                     \u001b[0mcmd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclasspath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_classpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstdout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mPIPE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstderr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mPIPE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    267\u001b[0m                 )\n\u001b[1;32m    268\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/nltk/internals.py\u001b[0m in \u001b[0;36mjava\u001b[0;34m(cmd, classpath, stdin, stdout, stderr, blocking)\u001b[0m\n\u001b[1;32m    138\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mblocking\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 140\u001b[0;31m     \u001b[0;34m(\u001b[0m\u001b[0mstdout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstderr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcommunicate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    141\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m     \u001b[0;31m# Check the return code.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/subprocess.py\u001b[0m in \u001b[0;36mcommunicate\u001b[0;34m(self, input, timeout)\u001b[0m\n\u001b[1;32m    861\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    862\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 863\u001b[0;31m                 \u001b[0mstdout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstderr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_communicate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mendtime\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    864\u001b[0m             \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    865\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_communication_started\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/subprocess.py\u001b[0m in \u001b[0;36m_communicate\u001b[0;34m(self, input, endtime, orig_timeout)\u001b[0m\n\u001b[1;32m   1532\u001b[0m                         \u001b[0;32mraise\u001b[0m \u001b[0mTimeoutExpired\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morig_timeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1533\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1534\u001b[0;31m                     \u001b[0mready\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mselector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1535\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_timeout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mendtime\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morig_timeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/selectors.py\u001b[0m in \u001b[0;36mselect\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    374\u001b[0m             \u001b[0mready\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    375\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 376\u001b[0;31m                 \u001b[0mfd_event_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_poll\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpoll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    377\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mInterruptedError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    378\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mready\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for i in modified_who_candidates.keys():\n",
    "    tups = modified_who_candidates[i]\n",
    "    for j in tups:\n",
    "        sent_no = j[0]\n",
    "        sentence = text_sent[sent_no]\n",
    "        sdp = StanfordDependencyParser()\n",
    "        result = list(sdp.raw_parse(sentence))\n",
    "\n",
    "        dep_tree_dot_repr = [parse for parse in result][0].to_dot()\n",
    "        source = Source(dep_tree_dot_repr, filename=i+\"-\"+str(sent_no), format=\"png\")\n",
    "        source.view(filename=i+\"-\"+str(sent_no),directory='dep graphs/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow_hub as hub\n",
    "import tensorflow as tf\n",
    "tf.logging.set_verbosity(tf.logging.ERROR)\n",
    "embed = hub.Module(\"https://tfhub.dev/google/universal-sentence-encoder/1\")\n",
    "def run_gse_benchmark(sentence1, sentence2):\n",
    "    sts_input1 = tf.placeholder(tf.string, shape=(None))\n",
    "    sts_input2 = tf.placeholder(tf.string, shape=(None))\n",
    "\n",
    "    sts_encode1 = tf.nn.l2_normalize(embed(sts_input1))\n",
    "    sts_encode2 = tf.nn.l2_normalize(embed(sts_input2))\n",
    "        \n",
    "    sim_scores = tf.reduce_sum(tf.multiply(sts_encode1, sts_encode2), axis=1)\n",
    "    \n",
    "    with tf.Session() as session:\n",
    "        session.run(tf.global_variables_initializer())\n",
    "        session.run(tf.tables_initializer())\n",
    "      \n",
    "        [gse_sims] = session.run(\n",
    "            [sim_scores],\n",
    "            feed_dict={\n",
    "                sts_input1: sentence1,\n",
    "                sts_input2: sentence2\n",
    "            })\n",
    "    return gse_sims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size : 100000\n"
     ]
    }
   ],
   "source": [
    "#Infer sent\n",
    "import torch\n",
    "from models import InferSent\n",
    "model_version = 1\n",
    "MODEL_PATH = \"encoder/infersent%s.pkl\" % model_version\n",
    "params_model = {'bsize': 64, 'word_emb_dim': 300, 'enc_lstm_dim': 2048,\n",
    "                'pool_type': 'max', 'dpout_model': 0.0, 'version': model_version}\n",
    "model = InferSent(params_model)\n",
    "model.load_state_dict(torch.load(MODEL_PATH))\n",
    "# Keep it on CPU or put it on GPU\n",
    "use_cuda = False\n",
    "model = model.cuda() if use_cuda else model\n",
    "# If infersent1 -> use GloVe embeddings. If infersent2 -> use InferSent embeddings.\n",
    "W2V_PATH = 'dataset/GloVe/glove.840B.300d.txt' #if model_version == 1 else '../dataset/fastText/crawl-300d-2M.vec'\n",
    "model.set_w2v_path(W2V_PATH)\n",
    "# Load embeddings of K most frequent words\n",
    "model.build_vocab_k_words(K=100000)\n",
    "def cosine(u, v):\n",
    "    return np.dot(u, v) / (np.linalg.norm(u) * np.linalg.norm(v))\n",
    "def run_infer_sent(sentence1,sentence2):\n",
    "    return cosine(model.encode([sentence1],tokenize=True)[0], model.encode([sentence2],tokenize=True)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "lmtzr = WordNetLemmatizer()\n",
    "def get_verb_similarity(phrase):\n",
    "    words = nltk.word_tokenize(phrase)\n",
    "    for w in words:\n",
    "        if lmtzr.lemmatize(w) in vocab:\n",
    "            return 1;\n",
    "    sent_sim = 0;\n",
    "    for w in words:\n",
    "        for v in vocab:\n",
    "            #s = run_gse_benchmark([w],[v])\n",
    "            s = run_infer_sent(w,v)\n",
    "            if s > sent_sim:\n",
    "                sent_sim = s\n",
    "    return sent_sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_who_index(who,p1):\n",
    "    words = nltk.word_tokenize(who)\n",
    "    for w in words:\n",
    "        for i in range(0,len(p1.nodes)):\n",
    "            if p1.nodes[i]['word'] != None and p1.nodes[i]['word'].lower() == w.lower():\n",
    "                return p1.nodes[i]['address']\n",
    "    return -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_who_root(ind,p1):\n",
    "    for i in range(0,len(p1.nodes)):\n",
    "        dependencies = p1.nodes[i]['deps'].keys()\n",
    "        for d in dependencies:\n",
    "            if ind in p1.nodes[i]['deps'][d]:\n",
    "                return p1.nodes[i]['address']\n",
    "    return -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "common_Dependencies = ['nsubj','csubj','dobj','iobj','obj','subj','dep']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_phrase(ind,p1):\n",
    "    phrase = []\n",
    "    phrase.append(p1.root['address'])\n",
    "    phrase.append(p1.nodes[ind]['address'])\n",
    "    other_deps = p1.nodes[ind]['deps'].keys()\n",
    "    for d in other_deps:\n",
    "        for i in p1.nodes[ind]['deps'][d]:\n",
    "            #considering all the right siblings\n",
    "            if i > ind:\n",
    "                for cd in common_Dependencies:\n",
    "                    if d in cd:\n",
    "                        phrase.append(i)\n",
    "    phrase.append(get_right_node(max(phrase),p1))\n",
    "    return sorted(list(set(phrase)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_right_node(ind,p1):\n",
    "    node = p1.nodes[ind]\n",
    "    while(len(node['deps'])!=0):\n",
    "        a= []\n",
    "        for i in node['deps'].values():\n",
    "            a = a+i\n",
    "        ind = max(a)\n",
    "        node = p1.nodes[ind]\n",
    "    return ind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Generate dependency trees for each of the sentence in which a who candidate appears.\n",
    "import os\n",
    "from nltk.parse.stanford import StanfordDependencyParser\n",
    "from graphviz import Source\n",
    "\n",
    "# make sure nltk can find stanford-parser\n",
    "# please check your stanford-parser version from brew output (in my case 3.6.0) \n",
    "os.environ['CLASSPATH'] = r'/usr/local/Cellar/stanford-parser/3.9.1/libexec'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_what_phrase(who,who_sent_number,text_sent,all_words):\n",
    "    sdp = StanfordDependencyParser()\n",
    "    result = sdp.raw_parse(text_sent[who_sent_number])\n",
    "    p1 = 0\n",
    "    for p in result:\n",
    "        p1 = p\n",
    "    ind = get_who_index(who,p1)\n",
    "    ind = get_who_root(ind,p1)\n",
    "    li = build_phrase(ind,p1)\n",
    "    phr = \"\"\n",
    "    for i in range(min(li),max(li)+1):\n",
    "        phr = phr+all_words[i]+\" \"\n",
    "    return phr.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_what_candidates(modified_who_candidates,text_sent,all_words):\n",
    "    what_candidates = {}\n",
    "    for w in modified_who_candidates.keys():\n",
    "        for t in modified_who_candidates[w]:\n",
    "            p = get_what_phrase(w,t[0],text_sent,all_words)\n",
    "            what_candidates[p] = t[0]\n",
    "    return what_candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_what_sim_scores(what_candidates):\n",
    "    sim_scores = {k:0 for k in what_candidates.keys()}\n",
    "    for w in what_candidates.keys():\n",
    "        sim_scores[w] = get_verb_similarity(w)\n",
    "    return sim_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# provide best who and where sent  numebers as input\n",
    "def get_distance_to_where_who(what_candidates, best_who, best_where,text_sent):\n",
    "    dist_scores = {k:0 for k in what_candidates.keys()}\n",
    "    for d in dist_scores.keys():\n",
    "        dist_scores[d] = ((1 - abs(what_candidates[d] - best_who)/len(text_sent)) + (1 - abs(what_candidates[d] - best_where)/len(text_sent)))/2\n",
    "    return dist_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:2: DeprecationWarning: The StanfordDependencyParser will be deprecated\n",
      "Please use \u001b[91mnltk.parse.corenlp.CoreNLPDependencyParser\u001b[0m instead.\n",
      "  \n",
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:2: DeprecationWarning: The StanfordDependencyParser will be deprecated\n",
      "Please use \u001b[91mnltk.parse.corenlp.CoreNLPDependencyParser\u001b[0m instead.\n",
      "  \n",
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:2: DeprecationWarning: The StanfordDependencyParser will be deprecated\n",
      "Please use \u001b[91mnltk.parse.corenlp.CoreNLPDependencyParser\u001b[0m instead.\n",
      "  \n",
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:2: DeprecationWarning: The StanfordDependencyParser will be deprecated\n",
      "Please use \u001b[91mnltk.parse.corenlp.CoreNLPDependencyParser\u001b[0m instead.\n",
      "  \n",
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:2: DeprecationWarning: The StanfordDependencyParser will be deprecated\n",
      "Please use \u001b[91mnltk.parse.corenlp.CoreNLPDependencyParser\u001b[0m instead.\n",
      "  \n",
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:2: DeprecationWarning: The StanfordDependencyParser will be deprecated\n",
      "Please use \u001b[91mnltk.parse.corenlp.CoreNLPDependencyParser\u001b[0m instead.\n",
      "  \n",
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:2: DeprecationWarning: The StanfordDependencyParser will be deprecated\n",
      "Please use \u001b[91mnltk.parse.corenlp.CoreNLPDependencyParser\u001b[0m instead.\n",
      "  \n",
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:2: DeprecationWarning: The StanfordDependencyParser will be deprecated\n",
      "Please use \u001b[91mnltk.parse.corenlp.CoreNLPDependencyParser\u001b[0m instead.\n",
      "  \n",
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:2: DeprecationWarning: The StanfordDependencyParser will be deprecated\n",
      "Please use \u001b[91mnltk.parse.corenlp.CoreNLPDependencyParser\u001b[0m instead.\n",
      "  \n",
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:2: DeprecationWarning: The StanfordDependencyParser will be deprecated\n",
      "Please use \u001b[91mnltk.parse.corenlp.CoreNLPDependencyParser\u001b[0m instead.\n",
      "  \n",
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:2: DeprecationWarning: The StanfordDependencyParser will be deprecated\n",
      "Please use \u001b[91mnltk.parse.corenlp.CoreNLPDependencyParser\u001b[0m instead.\n",
      "  \n",
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:2: DeprecationWarning: The StanfordDependencyParser will be deprecated\n",
      "Please use \u001b[91mnltk.parse.corenlp.CoreNLPDependencyParser\u001b[0m instead.\n",
      "  \n",
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:2: DeprecationWarning: The StanfordDependencyParser will be deprecated\n",
      "Please use \u001b[91mnltk.parse.corenlp.CoreNLPDependencyParser\u001b[0m instead.\n",
      "  \n",
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:2: DeprecationWarning: The StanfordDependencyParser will be deprecated\n",
      "Please use \u001b[91mnltk.parse.corenlp.CoreNLPDependencyParser\u001b[0m instead.\n",
      "  \n",
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:2: DeprecationWarning: The StanfordDependencyParser will be deprecated\n",
      "Please use \u001b[91mnltk.parse.corenlp.CoreNLPDependencyParser\u001b[0m instead.\n",
      "  \n",
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:2: DeprecationWarning: The StanfordDependencyParser will be deprecated\n",
      "Please use \u001b[91mnltk.parse.corenlp.CoreNLPDependencyParser\u001b[0m instead.\n",
      "  \n",
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:2: DeprecationWarning: The StanfordDependencyParser will be deprecated\n",
      "Please use \u001b[91mnltk.parse.corenlp.CoreNLPDependencyParser\u001b[0m instead.\n",
      "  \n",
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:2: DeprecationWarning: The StanfordDependencyParser will be deprecated\n",
      "Please use \u001b[91mnltk.parse.corenlp.CoreNLPDependencyParser\u001b[0m instead.\n",
      "  \n",
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:2: DeprecationWarning: The StanfordDependencyParser will be deprecated\n",
      "Please use \u001b[91mnltk.parse.corenlp.CoreNLPDependencyParser\u001b[0m instead.\n",
      "  \n",
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:2: DeprecationWarning: The StanfordDependencyParser will be deprecated\n",
      "Please use \u001b[91mnltk.parse.corenlp.CoreNLPDependencyParser\u001b[0m instead.\n",
      "  \n",
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:2: DeprecationWarning: The StanfordDependencyParser will be deprecated\n",
      "Please use \u001b[91mnltk.parse.corenlp.CoreNLPDependencyParser\u001b[0m instead.\n",
      "  \n",
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:2: DeprecationWarning: The StanfordDependencyParser will be deprecated\n",
      "Please use \u001b[91mnltk.parse.corenlp.CoreNLPDependencyParser\u001b[0m instead.\n",
      "  \n",
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:2: DeprecationWarning: The StanfordDependencyParser will be deprecated\n",
      "Please use \u001b[91mnltk.parse.corenlp.CoreNLPDependencyParser\u001b[0m instead.\n",
      "  \n",
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:2: DeprecationWarning: The StanfordDependencyParser will be deprecated\n",
      "Please use \u001b[91mnltk.parse.corenlp.CoreNLPDependencyParser\u001b[0m instead.\n",
      "  \n",
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:2: DeprecationWarning: The StanfordDependencyParser will be deprecated\n",
      "Please use \u001b[91mnltk.parse.corenlp.CoreNLPDependencyParser\u001b[0m instead.\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "# get what candidates\n",
    "# get what sim scores\n",
    "# get what distance to best where, who scores\n",
    "what_candidates = get_what_candidates(modified_who_candidates,text_sent,all_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{', Mr Anna Hazare , began a': 0,\n",
       " ', Mr Anna Hazare , began a fast unto death in the Capital on Tuesday demanding': 12,\n",
       " 'Bill Veteran Gandhian': 5,\n",
       " 'Gandhian , Mr Anna Hazare , began a fast unto death in': 5,\n",
       " 'Hazare , began a fast unto death in the Capital on Tuesday demanding': 1,\n",
       " 'Lokpal Bill Veteran Gandhian ,': 12,\n",
       " 'Lokpal Bill to check corruption .': 3,\n",
       " 'Mr Anna Hazare , began a': 4,\n",
       " 'corruption . Ignoring an': 3,\n",
       " 'death demanding Jan Lokpal Bill Veteran Gandhian , Mr Anna Hazare , began a fast unto': 2,\n",
       " 'death demanding Jan Lokpal Bill Veteran Gandhian , Mr Anna Hazare , began a fast unto death in the Capital on Tuesday demanding': 2,\n",
       " 'demanding Jan Lokpal Bill Veteran Gandhian ,': 6,\n",
       " 'fast unto': 9,\n",
       " 'fast unto death demanding Jan Lokpal Bill Veteran Gandhian , Mr Anna Hazare , began': 10,\n",
       " 'fast unto death demanding Jan Lokpal Bill Veteran Gandhian , Mr Anna Hazare , began a fast unto death in the': 10,\n",
       " 'fast unto death demanding Jan Lokpal Bill Veteran Gandhian , Mr Anna Hazare , began a fast unto death in the Capital on Tuesday': 10,\n",
       " 'on fast unto death demanding Jan': 8,\n",
       " 'on fast unto death demanding Jan Lokpal Bill Veteran Gandhian , Mr Anna Hazare , began a': 0,\n",
       " 'on fast unto death demanding Jan Lokpal Bill Veteran Gandhian , Mr Anna Hazare , began a fast unto death in the Capital on Tuesday demanding enactment of a': 10,\n",
       " 'the Capital on': 7,\n",
       " 'unto death demanding Jan Lokpal Bill Veteran Gandhian , Mr Anna Hazare , began a fast unto': 9}"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "what_candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sim_scores = get_what_sim_scores(what_candidates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{', Mr Anna Hazare , began a': 0.85450405,\n",
       " ', Mr Anna Hazare , began a fast unto death in the Capital on Tuesday demanding': 1,\n",
       " 'Bill Veteran Gandhian': 0.99999994,\n",
       " 'Gandhian , Mr Anna Hazare , began a fast unto death in': 1,\n",
       " 'Hazare , began a fast unto death in the Capital on Tuesday demanding': 1,\n",
       " 'Lokpal Bill Veteran Gandhian ,': 0.99999994,\n",
       " 'Lokpal Bill to check corruption .': 1,\n",
       " 'Mr Anna Hazare , began a': 0.85450405,\n",
       " 'corruption . Ignoring an': 1,\n",
       " 'death demanding Jan Lokpal Bill Veteran Gandhian , Mr Anna Hazare , began a fast unto': 1,\n",
       " 'death demanding Jan Lokpal Bill Veteran Gandhian , Mr Anna Hazare , began a fast unto death in the Capital on Tuesday demanding': 1,\n",
       " 'demanding Jan Lokpal Bill Veteran Gandhian ,': 0.99999994,\n",
       " 'fast unto': 0.6805536,\n",
       " 'fast unto death demanding Jan Lokpal Bill Veteran Gandhian , Mr Anna Hazare , began': 1,\n",
       " 'fast unto death demanding Jan Lokpal Bill Veteran Gandhian , Mr Anna Hazare , began a fast unto death in the': 1,\n",
       " 'fast unto death demanding Jan Lokpal Bill Veteran Gandhian , Mr Anna Hazare , began a fast unto death in the Capital on Tuesday': 1,\n",
       " 'on fast unto death demanding Jan': 1,\n",
       " 'on fast unto death demanding Jan Lokpal Bill Veteran Gandhian , Mr Anna Hazare , began a': 1,\n",
       " 'on fast unto death demanding Jan Lokpal Bill Veteran Gandhian , Mr Anna Hazare , began a fast unto death in the Capital on Tuesday demanding enactment of a': 1,\n",
       " 'the Capital on': 0.87522507,\n",
       " 'unto death demanding Jan Lokpal Bill Veteran Gandhian , Mr Anna Hazare , began a fast unto': 1}"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sim_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#this is a test\n",
    "dist_scores = get_distance_to_where_who(what_candidates,2,3,text_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{', Mr Anna Hazare , began a': 0.8076923076923077,\n",
       " ', Mr Anna Hazare , began a fast unto death in the Capital on Tuesday demanding': 0.2692307692307692,\n",
       " 'Bill Veteran Gandhian': 0.8076923076923077,\n",
       " 'Gandhian , Mr Anna Hazare , began a fast unto death in': 0.8076923076923077,\n",
       " 'Hazare , began a fast unto death in the Capital on Tuesday demanding': 0.8846153846153846,\n",
       " 'Lokpal Bill Veteran Gandhian ,': 0.2692307692307692,\n",
       " 'Lokpal Bill to check corruption .': 0.9615384615384616,\n",
       " 'Mr Anna Hazare , began a': 0.8846153846153846,\n",
       " 'corruption . Ignoring an': 0.9615384615384616,\n",
       " 'death demanding Jan Lokpal Bill Veteran Gandhian , Mr Anna Hazare , began a fast unto': 0.9615384615384616,\n",
       " 'death demanding Jan Lokpal Bill Veteran Gandhian , Mr Anna Hazare , began a fast unto death in the Capital on Tuesday demanding': 0.9615384615384616,\n",
       " 'demanding Jan Lokpal Bill Veteran Gandhian ,': 0.7307692307692307,\n",
       " 'fast unto': 0.5,\n",
       " 'fast unto death demanding Jan Lokpal Bill Veteran Gandhian , Mr Anna Hazare , began': 0.4230769230769231,\n",
       " 'fast unto death demanding Jan Lokpal Bill Veteran Gandhian , Mr Anna Hazare , began a fast unto death in the': 0.4230769230769231,\n",
       " 'fast unto death demanding Jan Lokpal Bill Veteran Gandhian , Mr Anna Hazare , began a fast unto death in the Capital on Tuesday': 0.4230769230769231,\n",
       " 'on fast unto death demanding Jan': 0.5769230769230769,\n",
       " 'on fast unto death demanding Jan Lokpal Bill Veteran Gandhian , Mr Anna Hazare , began a': 0.8076923076923077,\n",
       " 'on fast unto death demanding Jan Lokpal Bill Veteran Gandhian , Mr Anna Hazare , began a fast unto death in the Capital on Tuesday demanding enactment of a': 0.4230769230769231,\n",
       " 'the Capital on': 0.6538461538461539,\n",
       " 'unto death demanding Jan Lokpal Bill Veteran Gandhian , Mr Anna Hazare , began a fast unto': 0.5}"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dist_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Jan Lokpal Bill: Protests continue in support of Anna Hazare in Punjab\\n\\nCHANDIGARH: Khap panchayats today joined the protests in support of Anna Hazare in Punjab and Haryana where the Gandhian\\'s supporters took out car and bike rallies and staged a demonstration outside Prime Minister Manmohan Singh\\'s private residence here.\\xa0\\n\\nAs the protests spread, people in rural areas were seen participating in demonstrations in support of Hazare.\\xa0\\n\\nKhap spokesman Suresh Koth said the khap panchayats (caste councils) in Haryana are observing a hunger strike in Jind, the Jat heartland of Haryana, and other places in support of Hazare\\'s movement.\\xa0\\n\\nIn Chandigarh, activists of various organisations staged a protest outside the Prime Minister\\'s private residence in Sector 11.\\xa0\\n\\nThe Chandigarh administration had yesterday promulgated prohibitory orders banning assembly of five or more persons outside the residences of five VIPs, including that of the Prime Minister.\\xa0\\n\\nThe Additional District Magistrate had imposed the orders within hours of Hazare\\'s supporters protesting outside the residence of Union Parliamentary Affairs Minister Pawan Bansal in Sector 28 yesterday.\\xa0\\n\\nRepresentatives of many NGOs are observing a fast in the city besides former Union Minister Harmohan Dhawan said.\\xa0\\n\\n\"The protest will continue till the government comes out with a strong Jan Lok Pal Bill to save the country from the menace of corruption,\" an activist said.\\xa0\\n\\nAt several places in the two states, youth were seen holding a box and seeking donations for Hazare\\'s movement whereas at other places people took out car and bike rallies.\\xa0\\n\\n'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3, 4, 5]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "max(np.array([[1,2],[3,4,5]]).ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
